\documentclass[12pt]{report}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[italian]{babel}
\usepackage{bytefield}
\usepackage{cancel}
\usepackage{caption}
% \usepackage{embedall}\embedfile{\jobname.tex}
\usepackage{float}
\usepackage[bookmarks]{hyperref}
\usepackage{listings}
\usepackage[scr=rsfs]{mathalpha}
\usepackage{siunitx}
\usepackage{subcaption}
\usepackage{tabularray}
\usepackage[most]{tcolorbox}
\usepackage{pgfplots}
\usepgfplotslibrary{fillbetween}
\usepackage{tikz}\usetikzlibrary{automata, chains, scopes, decorations.text, patterns, patterns.meta, decorations.pathmorphing, positioning, decorations.pathreplacing, calligraphy, math, fit, shapes.geometric, pgfplots.fillbetween}
\usetikzlibrary{external}
\usepackage{todonotes}
\usepackage{xcolor}

\newtheorem{teorema}{Teorema}
\newtheorem{corollario}{Corollario}
\newtheorem{proposizione}{Proposizione}
\newtheorem{proprietà}{Proprietà}
\newtheorem{lemma}{Lemma}
\newtheorem{fatto}{Fatto}
\theoremstyle{definition}
\newtheorem{definizione}{Definizione}
% \theoremstyle{regard}
\newtheorem{nota}{Nota}

\renewcommand\qedsymbol{$\blacksquare$}

% \usepackage{teolang}

\definecolor{codegray}{gray}{0.95}

\lstdefinestyle{mystyle}{
  numberstyle=\tiny,
  basicstyle=\footnotesize,
  breaklines=true,
  numbers=left,
  numbersep=5pt,
}
\lstset{style=mystyle}

\overfullrule=0.2cm

\tikzexternalize 
\tcbset{shield externalize}

\begin{document}
\tableofcontents
\newpage

\chapter{Intro}
Gli automi a pila sono automi che, oltre ad avere un controllo a stati finiti, hanno una memoria arbitrariamente grande organizzata -- appunto -- pila; cioè una memoria a cui si può accedere solo all'elemento più in cima.
Il modello di cui tratteremo principalmente è quello non-deterministico e one-way sul nastro di input.
Infatti due risultati che mostreremo sono che:
\begin{itemize}
	\item la versione two-way è più potente (Sezione \ref{sec:Automi a pila two-way});
	\item il modello deterministico è -- a differenza degli FSA -- meno potente di quello nondeterministico (Sezione \ref{sec:???}).
\end{itemize}

\begin{definizione}[Automa a pila]
	\label{def:Automa a pila}
	Un automa a pila è una tupla
	$$ M = \langle Q, \Sigma, \Gamma, \delta, q_0, Z_0, F \rangle $$
	dove
	\begin{itemize}
		\item $\Gamma$ è l'alfabeto della pila o alfabeto di lavoro
		\item $\delta$ è la funzione di transizione
		\item $q_0 \in Q$ è lo stato iniziale dell'automa
		\item $Z_0 \in \Gamma$ è lo stato iniziale della pila
		\item $F$ è un insieme di stati finali
	\end{itemize}
	La funzione di transizione dipende da tre cose: dallo stato corrente, dal simbolo dell'input corrente e dal simbolo in cima alla pila
	$$ \delta : Q \times (\Sigma \cup \{\varepsilon\}) \times \Gamma \rightarrow \text{PF}(Q \times \Gamma^*) $$
	La funzione di transizione contemporaneamente cambia lo stato dell'automa e rimpiazza il simbolo in cima alla pila con una stringa di stati della pila\footnote{$\text{PF}(-)$ sta per le parti finite, infatti se utilizzassimo $2^{Q \times \Gamma^*}$ potremmo avere programmi infiniti, visto che $\Gamma^*$ è un insieme infinito.} \footnote{Scriviamo $\Sigma \cup \{ \varepsilon \}$ perché sono contemplate mosse in base allo stato dell'automa che modificano la pila senza leggere un simbolo in input.} \footnote{Per convenzione la stringa di stati viene messa sulla pila da destra a sinistra, quindi il simbolo più a sinistra sarà in cima alla pila.}.
\end{definizione}
% fig 12.1
\begin{figure}[H]
	\centering
	\begin{tikzpicture}[ SQUIGGLY/.style={->
			  		     , decorate
			                     , decoration={snake,amplitude=.4mm,segment length=2mm,post length=1mm}},
			   ]
 		\begin{scope}[local bounding box=wordScope, start chain=word, node distance=0pt]
    			\node [draw, minimum width=40pt, minimum height=20pt, on chain=word] {$\dots$};
			\node [draw, minimum height=20pt, on chain=word] {$a$};
    			\node [draw, minimum width=40pt, minimum height=20pt, on chain=word] {$\dots$};
 		\end{scope}

		\node[draw, minimum width=20pt, minimum height=20pt] (state) [below=of word-2]	{$q$};

		\begin{scope}[local bounding box=stackScope, start chain=stack going below, node distance=0pt]
			\node [draw, minimum width=20pt, on chain=stack] [right=of state, xshift=1cm] {$A$};
    			\node [draw, minimum width=20pt, minimum height=40pt, on chain=stack] {$\vdots$};
 		\end{scope}

		\node (a) [above=of stack-1, yshift=-1cm] {\tiny Pila};
		\node (b) [above=of word-2, yshift=-1cm] {\tiny Parola};
		\node (c) [below=of state, yshift=1cm] {\tiny Stato};

		\draw[SQUIGGLY] (state.north) to (word-2.south);
		\draw[SQUIGGLY] (state.east) to (stack-1.west);
	\end{tikzpicture}
	\caption{Rappresentazione delle varie parti di un PDA}
\end{figure}
% Visto che sono ammesse $\varepsilon$-mosse il modello di sopra potrebbe non esaurire tutte le possibilità.

\begin{tcolorbox}[breakable]
\label{ex:1}
Supponiamo di essere nello stato 
\begin{center}
	\begin{tikzpicture}[ SQUIGGLY/.style={->
			  		     , decorate
			                     , decoration={snake,amplitude=.4mm,segment length=2mm,post length=1mm}},
			   ]
 		\begin{scope}[local bounding box=wordScope, start chain=word, node distance=0pt]
    			\node [draw, minimum width=40pt, minimum height=20pt, on chain=word] {$\dots$};
    			\node [draw, minimum height=20pt, on chain=word] {$a$};
    			\node [draw, minimum width=40pt, minimum height=20pt, on chain=word] {$\dots$};
 		\end{scope}

		\node[draw, minimum width=20pt, minimum height=20pt] (state) [below=of word-2]	{$q$};

		\begin{scope}[local bounding box=stackScope, start chain=stack going below, node distance=0pt]
			\node [draw, minimum width=20pt, on chain=stack] [right=of state, xshift=1cm] {$A$};
    			\node [draw, minimum width=20pt, minimum height=40pt, on chain=stack] {$\vdots$};
 		\end{scope}

		\draw[SQUIGGLY] (state.north) to (word-2.south);
		\draw[SQUIGGLY] (state.east) to (stack-1.west);
	\end{tikzpicture}
\end{center}
e che la funzione $\delta$ sia così definita
$$\delta(q, a, A) = \{(q_1, \varepsilon), (q_2, BCC)\} $$
L'applicazione delle due alternative porterebbe l'automa nei seguenti stati
\begin{figure}[H]
	\centering
	\begin{subfigure}{0.4\textwidth}
		\centering
		\begin{tikzpicture}[ SQUIGGLY/.style={->
			, decorate
			, decoration={snake,amplitude=.4mm,segment length=2mm,post length=1mm}},
			]
			\begin{scope}[local bounding box=wordScope, start chain=word, node distance=0pt]
				\node [draw, minimum width=40pt, minimum height=20pt, on chain=word] {$\dots$};
				\node [draw, minimum height=20pt, on chain=word] {$a$};
				\node [draw, minimum height=20pt, on chain=word] {$?$};
				\node [draw, minimum width=40pt, minimum height=20pt, on chain=word] {$\dots$};
			\end{scope}

			\node[draw, minimum width=20pt, minimum height=20pt] (state) [below=of word-3]	{$q_1$};

			\begin{scope}[local bounding box=stackScope, start chain=stack going below, node distance=0pt]
				\node [draw, minimum width=20pt, minimum height=40pt, on chain=stack] [right=of state, xshift=1cm] {$\vdots$};
			\end{scope}

			\draw[SQUIGGLY] (state.north) to (word-3.south);
			\draw[SQUIGGLY] (state.east) to (stack-1.west);
		\end{tikzpicture}
		\caption{Lo stato per $(q_1, \varepsilon)$}
	\end{subfigure}
	\begin{subfigure}{0.4\textwidth}
		\centering
		\begin{tikzpicture}[ SQUIGGLY/.style={->
			, decorate
			, decoration={snake,amplitude=.4mm,segment length=2mm,post length=1mm}},
			]
			\begin{scope}[local bounding box=wordScope, start chain=word, node distance=0pt]
				\node [draw, minimum width=40pt, minimum height=20pt, on chain=word] {$\dots$};
				\node [draw, minimum height=20pt, on chain=word] {$a$};
				\node [draw, minimum height=20pt, on chain=word] {$?$};
				\node [draw, minimum width=40pt, minimum height=20pt, on chain=word] {$\dots$};
			\end{scope}

			\node[draw, minimum width=20pt, minimum height=20pt] (state) [below=of word-3]	{$q_2$};

			\begin{scope}[local bounding box=stackScope, start chain=stack going below, node distance=0pt]
				\node [draw, minimum width=20pt, on chain=stack] [right=of state, xshift=1cm] {$B$};
				\node [draw, minimum width=20pt, on chain=stack] {$C$};
				\node [draw, minimum width=20pt, on chain=stack] {$C$};
				\node [draw, minimum width=20pt, minimum height=40pt, on chain=stack] {$\vdots$};
			\end{scope}

			\draw[SQUIGGLY] (state.north) to (word-3.south);
			\draw[SQUIGGLY] (state.east) to (stack-1.west);
		\end{tikzpicture}
		\caption{Lo stato per $(q_2, BCC)$}
	\end{subfigure}
\end{figure}
Inoltre potremmo anche avere $\varepsilon$-mosse, ad esempio $ \delta(q, \varepsilon, A) = \{(r, B)\} $, porterebbe l'automa nello stato
\begin{center}
	\begin{tikzpicture}[ SQUIGGLY/.style={->
			  		     , decorate
			                     , decoration={snake,amplitude=.4mm,segment length=2mm,post length=1mm}},
			   ]
 		\begin{scope}[local bounding box=wordScope, start chain=word, node distance=0pt]
    			\node [draw, minimum width=40pt, minimum height=20pt, on chain=word] {$\dots$};
    			\node [draw, minimum height=20pt, on chain=word] {$a$};
    			\node [draw, minimum width=40pt, minimum height=20pt, on chain=word] {$\dots$};
 		\end{scope}

		\node[draw, minimum width=20pt, minimum height=20pt] (state) [below=of word-2]	{$r$};

		\begin{scope}[local bounding box=stackScope, start chain=stack going below, node distance=0pt]
			\node [draw, minimum width=20pt, on chain=stack] [right=of state, xshift=1cm] {$B$};
    			\node [draw, minimum width=20pt, minimum height=40pt, on chain=stack] {$\vdots$};
 		\end{scope}

		\draw[SQUIGGLY] (state.north) to (word-2.south);
		\draw[SQUIGGLY] (state.east) to (stack-1.west);
	\end{tikzpicture}
\end{center}
\end{tcolorbox}

\section{Definizioni}\label{sect:def}
Ci riferiremo agli automi a pila come PDA (Push Down Automaton) e assumeremo che siano sempre nondeterministici, a meno che diversamente specificato.

Chiameremo lo stato complessivo dell'automa a pila la sua \textbf{configurazione}, questa verrà rappresentata compattamente come la tripla dello stato corrente, la porzione di input ancora da leggere, e il contenuto della pila.
Quindi 
% fig 12.3
\begin{center}
	\begin{tikzpicture}[ SQUIGGLY/.style={->
			  		     , decorate
			                     , decoration={snake,amplitude=.4mm,segment length=2mm,post length=1mm}},
			   ]
 		\begin{scope}[local bounding box=wordScope, start chain=word, node distance=0pt]
    			\node [draw, minimum width=40pt, minimum height=20pt, on chain=word] {$x$};
			\node [draw, minimum height=20pt, on chain=word] {$a$};
    			\node [draw, minimum width=40pt, minimum height=20pt, on chain=word] {$y$};
 		\end{scope}

		\node[draw, minimum width=20pt, minimum height=20pt] (state) [below=of word-2]	{$q$};

		\begin{scope}[local bounding box=stackScope, start chain=stack going below, node distance=0pt]
			\node [draw, minimum width=20pt, on chain=stack] [right=of state, xshift=1cm] {$A$};
    			\node [draw, minimum width=20pt, minimum height=40pt, on chain=stack] {$\alpha$};
 		\end{scope}

		\draw[SQUIGGLY] (state.north) to (word-2.south);
		\draw[SQUIGGLY] (state.east) to (stack-1.west);
	\end{tikzpicture}
\end{center}
è rappresentato dalla configurazione
$$ (q, ay, A\alpha) $$
con $q \in Q, a \in \Sigma \cup \{\varepsilon\}, y \in \Sigma^*, A \in \Gamma, \alpha \in \Gamma^*$.

Una mossa, scritto $q \vdash p$, indica che da una configurazione $q$ posso passare ad un'altra $p$.
Ad esempio nell'Esempio \ref{ex:1} abbiamo che
\begin{align*}
	(q, ay, A\alpha) &\vdash (q_1, y, \alpha) \\
	(q, ay, A\alpha) &\vdash (q_2, y, BCC\alpha) \\
	(q, ay, A\alpha) &\vdash (r, ay, B\alpha)
\end{align*}
Più rigorosamente, sia $(q, ay, Z\alpha)$ la configurazione corrente, con $Z \in \Gamma$ e $M$ l'automa a pila, diciamo che
$$ (q, ay, Z\alpha) \underset{M}{\vdash} (p, y, \beta\alpha) $$
sse $(p, \beta) \in \delta(q, a, Z)$ dove $q, p \in Q$, $y \in \Sigma^*$, $a \in \Sigma \cup \{\varepsilon\}$, $Z \in \Gamma$ e $\alpha, \beta \in \Gamma^*$.
Se l'automa è ovvio dal contesto possiamo ometterlo da $\underset{M}{\vdash}$ e scrivere solo $\vdash$.

Da una configurazione $C'$ arrivo ad una configurazione $C''$ in un certo numero di mosse -- scritto 
$$ C' \underset{M}{\overset{*}{\vdash}} C'' $$
sse esistono $C_0, \dots, C_k$ con $C_0 = C'$ e $C_k = C''$ e $\forall i \in 1, \dots, k \; C_{i - 1} \underset{M}{\vdash} C_i$.

La configurazione iniziale di un automa su input $w \in \Sigma^*$ è 
$$ (q_0, w, Z_0) $$

Per accettare possiamo dare alcune diverse definizioni di configurazione accettante:
\begin{itemize}
	\item una volta finito l'input mi trovo in uno stato $q \in F$ e la pila può essere una stringa qualunque, questa è detta \textit{accettazione per stati finali}\footnote{Siccome sono accettate le $\varepsilon$ mosse può esserci il caso in arriviamo alla fine dell'input con uno stato non finale, e si può fare una $\varepsilon$-mossa ed arrivare ad uno stato finale.}
			, ed indichiamo il linguaggio accettato per stati finali dall'automa a pila $M$ come
		$$ L(M) = \{ w \in \Sigma^* \mid (q_0, w, Z_0) \overset{*}{\vdash} (q, \varepsilon, \gamma), q \in F, \gamma \in \Gamma^* \} $$
	\item è ragionevole pensare che tutto quello che viene messo sulla pila debba anche essere tolto, questa è detta \textit{accettazione per pila vuota} per cui si deve arrivare alla fine dell'input ed aver svuotato l'intera pila, ignorando lo stato.
		Il linguaggio accettato per pila vuota dall'automa $M$ lo indichiamo come
		$$ N(M) = \{ w \in \Sigma^* \mid (q_0, w, Z_0) \overset{*}{\vdash} (q, \varepsilon, \varepsilon), q \in Q \} $$
		In questo caso ovviamente si può omettere $F$ dalla definizione dell'automa.
	\item si può pensare di richiedere entrambe le precedenti, come vedremo più avanti queste tre nozioni sono equivalenti nel caso nondeterministico (Sezione \ref{sect:eq-nondet}).
\end{itemize}
\begin{nota} % sistemo
	Questa cosa la vedremo meglio, ma visto che la pila è la struttura fondamentale per la ricorsione, i linguaggi CF sono i linguaggi regolari a cui è stata aggiunta la ricorsione.
\end{nota}

\begin{tcolorbox}
	Definiamo il linguaggio
	$$ \mathcal{L} = \{ a^n b^n \mid n \geq 1 \} $$
	possiamo usare la pila per contare il numero di $a$.
	\begin{align*}
		\delta(q_0, a, Z_0) &= \{(q_0, A)\} \\
		\delta(q_0, a, A)   &= \{(q_0, AA) \} \\
		\delta(q_0, b, A)   &= \{ (q_1, \varepsilon) \} \\
		\delta(q_1, b, A)   &= \{ (q_1, \varepsilon) \} 
	\end{align*}
	E vale che data questa $\delta$
	$$ \mathcal{L} = N(M) $$
	Questo caso particolare di automa a pila in cui utilizziamo in simbolo solo (cioè $A$, oltre a $Z_0$) è detto \textit{automa a contatore}.

	Definiamo alternativamente
	\begin{align*}
		\delta(q_0, a, Z_0) &= \{(q_0, AZ_0)\} \\
		\delta(q_0, a, A)   &= \{(q_0, AA)\} \\
		\delta(q_0, b, A)   &= \{(q_1, \varepsilon)\} \\
		\delta(q_1, b, A)   &= \{(q_1, \varepsilon)\} \\
		\delta(q_1, \varepsilon, Z_0) &= \{(q_F, \varepsilon)\}
	\end{align*}
	con $F = \{q_F\}$, e vale che con questa $\delta$
	$$ \mathcal{L} = L(M) $$

	% sistemo wording
	Vediamo ora il caso di sopra, ma in cui
	$$ \mathcal{L} = \{ a^n b^n \mid n \geq 0 \} $$
	possiamo usare la pila per contare il numero di $a$.
	\begin{align*}
		\delta(q_0, \varepsilon, Z_0) &= \{(q_0, \varepsilon)\} \\
		\delta(q_0, a, Z_0) &= \{(q_0, A)\} \\
		\delta(q_0, a, A)   &= \{(q_0, AA) \} \\
		\delta(q_0, b, A)   &= \{ (q_1, \varepsilon) \} \\
		\delta(q_1, b, A)   &= \{ (q_1, \varepsilon) \} 
	\end{align*}
	E vale che data questa $\delta$ in cui si può direttamente accettare dallo stato $q_0$
	$$ \mathcal{L} = N(M) $$
	L'introduzione della prima regola è problematica, perché con input non vuoto permette di svuotare la pila da $Z_0$, bloccando la continuazione dell'automa, quindi abbiamo introdotto il nondeterminismo tra le due regole $\delta(q_0, \varepsilon, Z_0)$ e $\delta(q_0, a, Z_0)$.

	Definiamo similmente a sopra
	\begin{align*}
		\delta(q_0, \varepsilon, Z_0) &= \{(q_F, \varepsilon)\} \\
		\delta(q_0, a, Z_0) &= \{(q_0, AZ_0)\} \\
		\delta(q_0, a, A)   &= \{(q_0, AA)\} \\
		\delta(q_0, b, A)   &= \{(q_1, \varepsilon)\} \\
		\delta(q_1, b, A)   &= \{(q_1, \varepsilon)\} \\
		\delta(q_1, \varepsilon, Z_0) &= \{(q_F, \varepsilon)\}
	\end{align*}
	con $F = \{q_F\}$, e vale che con questa $\delta$
	$$ \mathcal{L} = L(M) $$
	sempre introducendo non determinismo.
	
	Questa versione, a differenza di quello di sopra per pila vuota, può anche essere fatta senza non determinismo infatti definiamo $q_I$ come nuovo stato iniziale che è anche finale, se la stringa è vuota possono direttamente accettare, mentre 
	$$ \delta(q_I, a, Z_0) = \{(q_0, AZ_0)\} $$
	ci riconduce all'automa di sopra.
\end{tcolorbox}

Definiamo ora l'automa a pila deterministico.
Questo in ogni configurazione permette una singola scelta:
\begin{itemize}
	\item sono vietate configurazioni che ammettono una mossa e una $\varepsilon$-mossa, quindi $\forall q \in Q, z \in \Gamma$ se $\delta(q, \varepsilon, z) \neq \varnothing$ allora $\forall a \in \Sigma \; \delta(q, a, Z) = \varnothing$
	\item per ogni tripletta $q, a, Z$ è ammessa al massimo una mossa, quindi 
		$$\forall q \in Q, z \in \Gamma, a \in \Sigma \cup \{\varepsilon\} \mid |\delta(q, a, Z)|\leq 1$$
\end{itemize}
% A questo punto abbiamo definito quattro modelli: deterministico e nondeterministico che possono accettare per pila vuota o per stato finale.
% Vedremo che il caso nondeterminismo in questo caso è più potente del caso deterministico, e che nel modello nondeterministico automi che possono accettare per pila vuota o per stato finale sono equivalenti.

\section{Equivalenza tra le due nozioni di accettazione nel modello nondeterministico}\label{sect:eq-nondet}
Dimostriamo ora che le due nozioni di PDA che abbiamo visto nella Sezione \ref{sect:def} sono equivalenti.
\begin{proof}[Da stati finali a pila vuota]
Dato un automa $M = (Q, \Sigma, \Gamma, \delta, q_0, Z_0, F)$ e supponiamo che $L = L(M)$ sia il linguaggio accettato per stati finali.
Definiamo l'automa 
$$M' = (Q \cup \{q_0', q_e\}, \Sigma, \Gamma \cup \{X\}, \delta', q_0', X, \varnothing)$$
con $q_0', q_e \not \in Q$ e $X \not \in \Gamma$, vogliamo che $L= N(M')$.

Ad alto livello quando $M$ arriva in uno stato finale, $M'$ si sposta nello stato $q_e$ in cui inizia a svuotare la pila.
Infatti la $e$ di $q_e$ sta per ``empty''.

Definiamo ora $\delta'$:
\begin{enumerate}
	\item prima di tutto 
		$$ \delta'(q_0', \varepsilon, X) = \{(q_0, Z_0X)\} $$
		questo serve solo ad infilare $X$ in fondo alla pila.
		La $X$ è necessaria per evitare che se l'automa iniziale $M$ svuota la pila si accetti la stringa.
	\item per ogni altra cosa $M'$ si può comportare come $M$:
		$$ \forall q \in Q, a \in \Sigma \cup \{\varepsilon\}, z \in \Gamma \mid \delta(q, a, Z) \subseteq \delta'(q, a, Z) $$
	\item ogni qualvolta $M$ entra in uno stato finale $M'$ può -- enfasi su può -- iniziare a svuotare l'intera pila:
		$$ \forall q \in F, z \in \Gamma \cup \{X\} \mid (q_e, \varepsilon) \in \delta'(q, \varepsilon, Z) $$
	\item una volta entrato nello stato di svuotamento, continua a svuotare:
		$$ \forall z \in \Gamma \cup \{X\} \mid \delta'(q_e, \varepsilon, Z) = \{(q_e, \varepsilon)\}$$ 
\end{enumerate}

Questo necessariamente introduce nondeterminismo, infatti l'automa $M$ potrebbe entrare in uno stato finale prima di essere arrivato alla fine della stringa.
Ed anche se l'automa di partenza è deterministico il punto $3$ potrebbe in ogni caso introdurre nondeterminismo.

Supponiamo di avere un automa deterministico che accetta la stringa $w$ a pila vuota, allora ogni stringa che ha $w$ come prefisso non può essere accettata, perché il prefisso $w$ svuoterebbe la pila e un automa con pila vuota non può andare a avanti.
Quindi il nondeterminismo è in un certo senso necessario per automi a pila che accettano con pila vuota.
\end{proof}
\begin{proof}[Da pila vuota a stati finali]
Dato un automa $M = (Q, \Sigma, \Gamma,\delta, q_0, Z_0, \varnothing)$ che accetta per pila vuota il linguaggio $L = N(M)$, vogliamo creare un automa che accetti per stati finali.
Sia questo 
$$M' = (Q \cup \{q_0', q_F\}, \Sigma, \Gamma \cup \{X\}, \delta', q_0', X, F = \{q_F\})$$
con $q_0', q_F \not \in Q, X \not \in \Gamma$.

Definiamo ora $\delta'$:
\begin{itemize}
	\item come prima inizialmente infiliamo $X$ in fondo alla pila:
		$$ \delta'(q_0', \varepsilon, X) = \{(q_0, Z_0X)\} $$
		$X$ serve a riconoscere quando la pila è vuota.
	\item a questo punto copiamo tutte le mosse di $M$, per cui
		$$ \forall q \in Q, a \in \Sigma \cup \{\varepsilon\}, Z \in \Gamma \mid \delta'(q, a, Z) = \delta(q, a, Z) $$
	\item nel momento in cui $M$ svuota la prima, $M'$ si trova $X$ sulla pila, a questo punto può entrare in uno stato finale
		$$ \forall q \in Q \mid \delta'(q, \varepsilon, X) = \{(q_F, \varepsilon)\}$$
\end{itemize}
Supponendo che $M$ sia deterministico, $M'$ rimane deterministico -- la trasformazione preserva il determinismo.
\end{proof}

% lezione 13
\chapter{Grammatiche di tipo 2}
Una grammatica è formata da quattro elementi:
$$ G = \langle V, \Sigma, P, S \rangle $$
e nello specifico, in quelle di tipo 2 le produzioni hanno la forma
$$ A \rightarrow \alpha \hspace{1cm} A \in V, \alpha \in (V \cup \Sigma)^* $$

Una rappresentazione utile per le derivazioni di linguaggi CF sono gli alberi, ad esempio data $w \in L(G)$, allora $S \overset{*}{\Rightarrow} w$.
Questa derivazione io la possono rappresentare come un albero di derivazione, o albero di parsing, o ancora parse tree.
Questo è un albero 
\begin{itemize}
	\item con radice etichettata con il simbolo iniziale della grammatica
	\item le foglie da sinistra a destra sono $w$
	\item i nodi possono essere di tre tipi:
		\begin{itemize}
			\item variabili, per i nodi interni
			\item terminali, per le foglie
			\item $\varepsilon$ la parola vuota, in casi speciali per le foglie
		\end{itemize}
\end{itemize}
Dato un nodo
\begin{center}
	\begin{tikzpicture}
		\node {$A$}
			child { node {$X_1$} }
			child { node {$X_2$} }
			child { node {$\dots$} }	% non disegno il nodo
			child { node {$X_k$} };
	\end{tikzpicture}
\end{center}
rappresenta l'applicazione della regola di produzione
$$ A \rightarrow X_1 X_2 \dots X_k \in P \hspace{1cm} A \in V, \forall i \in 1, \dots, k \mid X_i \in V \cup \Sigma $$
All'ultimo livello possiamo avere nodi
\begin{center}
	\begin{tikzpicture}
		\node {$A$}
		child { node {$\varepsilon$} };
	\end{tikzpicture}
\end{center}
solo se $A \rightarrow \varepsilon \in P$.

Abbiamo detto che gli automi a pila riconoscono linguaggi con ricorsione, dove questa nell'automa si esprime nella memoria a pila, nelle grammatiche si esprime nella struttura ad albero.
 
% \begin{tcolorbox}[breakable] 	% extra \else error with externalized tikzpicture
Definiamo la grammatica per le parentesi correttamente bilanciate
\begin{align*}
	S &\rightarrow \varepsilon \\
	S &\rightarrow ( S ) \\
	S &\rightarrow S S
\end{align*}
prendiamo ora la stringa $w = (())()()$ e scriviamone la derivazione
\begin{figure}[H]
	\centering
	\begin{subfigure}{0.3\textwidth}
		\centering
		\begin{tikzpicture}[scale=0.45, every node/.style={transform shape}]
			\node {$S$}
				[sibling distance=4.5cm]
				child { node {$S$} 
					[sibling distance=3cm]
					child { node {$S$} 
						[sibling distance=1cm]
						child { node {$($} }
						child { node {$S$} 
							[sibling distance=1cm]
							child { node {$($} }
							child { node {$S$} 
								child { node {$\varepsilon$} }
							}
							child { node {$)$} }
						}
						child { node {$)$} }
					}
					child { node {$S$}
						[sibling distance=1cm]
						child { node {$($} }
						child { node {$S$} 
							child { node {$\varepsilon$} }
						}
						child { node {$)$} }
					}
				}
				child { node {$S$} 
					[sibling distance=1cm]
					child { node {$($} }
					child { node {$S$} 
						child { node {$\varepsilon$} }
					}
					child { node {$)$} }
				};
		\end{tikzpicture}
		\begin{align*}
			S &\Rightarrow S S  \\
			&\Rightarrow S S S  \\
			&\Rightarrow ( S ) S S  \\
			&\Rightarrow ( S ) ( S ) S  \\
			&\Rightarrow ( S ) ( S ) ( S )  \\
			&\Rightarrow ( ( S ) ) ( S ) ( S )  \\
			&\Rightarrow ( ( S ) ) ( S ) ( )  \\
			&\Rightarrow ( ( S ) ) ( ) ( )  \\
			&\Rightarrow ( ( ) ) ( ) ( )  \\
		\end{align*}
	\end{subfigure}
	\begin{subfigure}{0.3\textwidth}
		\centering
		\begin{tikzpicture}[scale=0.45, every node/.style={transform shape}]
 			\node {$S$}
			[sibling distance=4.5cm]
 			child { node {$S$} 
				[sibling distance=1cm]
 				child { node {$($} }
 				child { node {$S$} 
					[sibling distance=1cm]
 					child { node {$($} }
 					child { node {$S$} 
 						child { node {$\varepsilon$} }
 					}
 					child { node {$)$} }
 				}
 				child { node {$)$} }
 			}
 			child { node {$S$} 
				[sibling distance=3cm]
 				child { node {$S$} 
					[sibling distance=1cm]
 					child { node {$($} }
 					child { node {$S$} 
 						child { node {$\varepsilon$} }
 					}
 					child { node {$)$} }
 				}
 				child { node {$S$}
					[sibling distance=1cm]
 					child { node {$($} }
 					child { node {$S$} 
 						child { node {$\varepsilon$} }
 					}
 					child { node {$)$} }
 				}
 			};
 		\end{tikzpicture}
		\begin{align*}
			S &\Rightarrow S S  \\
			  &\Rightarrow ( S ) S  \\
			  &\Rightarrow ( ( S ) ) S  \\
			  &\Rightarrow ( ( ) ) S  \\
			  &\Rightarrow ( ( ) ) S S  \\
			  &\Rightarrow ( ( ) ) ( S ) S  \\
			  &\Rightarrow ( ( ) ) ( ) S  \\
			  &\Rightarrow ( ( ) ) ( ) ( S )  \\
			  &\Rightarrow ( ( ) ) ( ) ( )  \\
		\end{align*}
	\end{subfigure}
	\begin{subfigure}{0.3\textwidth}
		\centering
		\begin{tikzpicture}[scale=0.45, every node/.style={transform shape}]
			\node {$S$}
				[sibling distance=4.5cm]
				child { node {$S$} 
					[sibling distance=3cm]
					child { node {$S$} 
						[sibling distance=1cm]
						child { node {$($} }
						child { node {$S$} 
							[sibling distance=1cm]
							child { node {$($} }
							child { node {$S$} 
								child { node {$\varepsilon$} }
							}
							child { node {$)$} }
						}
						child { node {$)$} }
					}
					child { node {$S$}
						[sibling distance=1cm]
						child { node {$($} }
						child { node {$S$} 
							child { node {$\varepsilon$} }
						}
						child { node {$)$} }
					}
				}
				child { node {$S$} 
					[sibling distance=1cm]
					child { node {$($} }
					child { node {$S$} 
						child { node {$\varepsilon$} }
					}
					child { node {$)$} }
				};
		\end{tikzpicture}
	 	\begin{align*}
	 		S &\Rightarrow S S \\
	 		  &\Rightarrow S S S \\
	 		  &\Rightarrow ( S ) S S \\
	 		  &\Rightarrow ( ( S ) ) S S \\
	 		  &\Rightarrow ( ( ) ) S S \\
	 		  &\Rightarrow ( ( ) ) ( S ) S \\
	 		  &\Rightarrow ( ( ) ) ( ) S \\
	 		  &\Rightarrow ( ( ) ) ( ) ( S ) \\
	 		  &\Rightarrow ( ( ) ) ( ) ( ) \\
	 	\end{align*}
	\end{subfigure}
	\caption{Tre derivazioni diverse per la stringa $(())()()$ e gli alberi corrispondenti}
\end{figure}
Possiamo vedere che una stessa stringa ammette diverse derivazioni, ma non tutte queste portano allo stesso albero.
Infatti la prima e la terza derivazione utilizzano le stesse sostituzioni, solo in ordine diverso, e quindi generano alberi uguali; mentre nel secondo albero applichiamo derivazioni diverse.
Nella prima e nella terza derivazione abbiamo una struttura
\begin{center}
	\begin{tikzpicture}
		\node {$S$}
		child { node {$S$} 
			child { node {$\Delta$} }
			child { node {$\Delta$} }
		}
		child { node {$\Delta$} };
	\end{tikzpicture}
\end{center}
mentre la seconda ha una struttura
\begin{center}
	\begin{tikzpicture}
		\node {$S$}
		child { node {$\Delta$} }
		child { node {$S$} 
			child { node {$\Delta$} }
			child { node {$\Delta$} }
		};
	\end{tikzpicture}
\end{center}
% \end{tcolorbox}

Per evitare derivazioni multiple si utilizza un criterio detto di \textit{derivazione leftmost}: una derivazione è lefmost se ogni volta che si fa una sostituzione sostituisco sempre la variabile più a sinistra della forma sentenziale.
\begin{proposizione}
	Esiste una corrispondenza uno a uno tra derivazioni leftmost e alberi di derivazione.
\end{proposizione}
La seconda e la terza derivazioni dell'esempio di sopra sono due derivazioni leftmost diverse.

\begin{definizione}
Diciamo che una grammatica è \textit{ambigua} se c'è una stringa che ammette almeno due alberi di derivazione -- o derivazioni leftmost -- diversi.
\end{definizione}

Nell'esempio di sopra si può vedere anche che ogni sottoalbero è una sequenza bilanciate di parentesi.

\begin{tcolorbox} % [breakable] 	% stesso problema di sopra
Se nella grammatica di sopra vorremmo anche le quadre, senza precedenze, questa è facilmente
\begin{align*}
 	S &\rightarrow \varepsilon \\
 	S &\rightarrow ( S ) \\
 	S &\rightarrow [ S ] \\
 	S &\rightarrow S S
\end{align*}
Ma se si chiede che le quadre non possano stare all'interno delle tonde, diventa necessario suddividere le variabili in due livelli
\begin{align*}
 	S &\rightarrow T \\
 	S &\rightarrow [ S ] \\
 	S &\rightarrow S S \\
 	T &\rightarrow \varepsilon \\
 	T &\rightarrow ( T ) \\
 	T &\rightarrow T T \\
\end{align*}
e vediamo un albero di derivazione di esempio
\begin{center}
	\begin{tikzpicture}
		\node {$S$}
		[sibling distance=1cm, level distance=1cm]
		child { node {$[$} }
		child { node {$S$} 
			[sibling distance=3cm]
			child { node {$S$} 
				[sibling distance=1cm]
				child { node {$T$} 
					child { node {$($} }
					child { node {$T$} 
						child { node {$\varepsilon$} }
					}
					child { node {$)$} }
				}
			}
			child { node {$S$} 
				[sibling distance=1cm]
				child { node {$[$} }
				child { node {$S$} 
					child { node {$T$}
						child { node {$\varepsilon$} }
					}
				}
				child { node {$]$} }
			}
		}
		child { node {$]$} };
	\end{tikzpicture}
\end{center}
\end{tcolorbox}


\section{Equivalenza tra grammatiche di tipo 2 ad automi a pila}
Mostriamo ora l'equivalenza tra le grammatiche di tipo 2 e gli automi a pila.
\begin{proof}[Da una grammatica che genera un linguaggio generiamo un automa che riconosce lo stesso]
Data una grammatica
$$ G = \langle V, \Sigma, P, S \rangle $$
di tipo 2, vogliamo costruire
$$ M = \langle Q, \Sigma, \Gamma, \delta, q, Z_0, \varnothing \rangle $$
che accetta per pila vuota, con
\begin{itemize}
 	\item $Q$ formato da un solo stato $\{q\}$
 	\item $\Gamma = \Sigma \cup V$
 	\item $Z_0 = S$
\end{itemize}
e $\delta$ definito come
\begin{itemize}
 	\item se $A \rightarrow \alpha \in P$ allora $(q, \alpha) \in \delta(q, \varepsilon, A)$
 	\item $\forall \sigma \in \Sigma$, $\delta(q, \sigma, \sigma) = \{ (q, \varepsilon) \}$, cioè si consuma il simbolo in cima alla pila
\end{itemize}
 
Si può dimostrare che il linguaggio generato dalla grammatica $L(G)$ è uguale al linguaggio accettato dall'automa per pila vuota $N(M)$.
\end{proof}

\begin{tcolorbox}[breakable]
 	Prendiamo
 	$$ G = \langle \{S, T, U\}, \{a, b\}, P, S \rangle $$
 	con $P$ definito
 	\begin{align*}
 		S &\rightarrow TU  \\
 		T &\rightarrow a T b \mid \varepsilon \\
 		U &\rightarrow b U a \mid \varepsilon \\
 	\end{align*}
 	questo genera
 	$$ L = \{ a^n b^{n + m} a^m \mid n \geq 0, m \geq 0 \} $$
 
 	Scriviamo le transizioni dell'automa corrispondente
 	$$ M = \langle \{q\}, \{a, b\}, \{S, T, U, a, b\}, \delta, q, S, \varnothing \rangle $$
 	con $\delta$ definito come
 	\begin{align*}
 		\delta(q, \varepsilon, S) &= \{(q, TU)\} \\
 		\delta(q, \varepsilon, T) &= \{(q, a T b), (q, \varepsilon) \} \\
 		\delta(q, \varepsilon, U) &= \{(q, b U a), (q, \varepsilon) \} \\
 		\delta(q, a, a) &= \{(q, \varepsilon)\} \\
 		\delta(q, b, b) &= \{(q, \varepsilon)\} \\
 	\end{align*}
 	\newpage
 	Prendendo per esempio $w = abbbaa$, vediamo come viene accettata nondeterministicamente
 	\begin{align*}
 		(q, abbbaa, S) &\vdash (q, abbbaa, TU) \\
 		               &\vdash (q, abbbaa, TU) \\
 		               &\vdash (q, abbbaa, aTbU) \\
 		               &\vdash (q, bbbaa, TbU) \\
 		               &\vdash (q, bbbaa, bU) \\
 		               &\vdash (q, bbaa, U) \\
 		               &\vdash (q, bbaa, bUa) \\
 		               &\vdash (q, baa, Ua) \\
		               &\vdash (q, baa, bUaa) \\
 		               &\vdash (q, aa, Uaa) \\
 		               &\vdash (q, aa, aa) \\
 		               &\vdash (q, a, a) \\
 		               &\vdash (q, \varepsilon, \varepsilon) \\
 	\end{align*}
 	Leggere la i terminali consumati fino a un certo punto e il contentuto della pila in quel punto restituisce la forma sentenziale durante la derivazione.
 	Questo corrisponde a
 	\begin{align*}
 		S &\Rightarrow T U \\
 		  &\Rightarrow a T b U \\
 		  &\Rightarrow a b U \\
 		  &\Rightarrow a b b U a \\
 		  &\Rightarrow a b b b U a a \\
 		  &\Rightarrow a b b b a a \\
 	\end{align*}
\end{tcolorbox}
L'automa a pila tenta di simulare il processo di derivazione leftmost della stringa.

Mostriamo ora il lato opposto dell'equivalenza, per fare questo però 
% iniziamo a introdurla
Per la dimostrazione useremo una variazione degli automi a pila che non ne cambia la potenza computazionale.
In questa forma normale
\begin{itemize}
	\item all'inizio la pila contiene un simbolo speciale $Z_0$ che viene mai rimosso e non viene mai aggiunto
 		% lez 13.1
		\begin{center}
			\begin{tikzpicture}[ SQUIGGLY/.style={->
				, decorate
				, decoration={snake,amplitude=.4mm,segment length=2mm,post length=1mm}},
				]
				\begin{scope}[local bounding box=wordScope, start chain=word, node distance=0pt]
					\node [draw, minimum width=25pt, minimum height=25pt, on chain=word] {$\sigma$};
					\node [draw, minimum width=65pt, minimum height=25pt, on chain=word] {$\dots$};
				\end{scope}

				\node[draw, minimum width=25pt, minimum height=25pt] (state) [below=of word-1] {$q_0$};
				\node[draw, minimum width=25pt, minimum height=25pt] (stack) [right=of state] {$Z_0$};

				\draw[SQUIGGLY] (state.north) to (word-1.south);
				\draw[SQUIGGLY] (state.east) to (stack.west);
			\end{tikzpicture}
		\end{center}
 	\item alla fine l'input è stato letto completamente, la pila contiene solo $Z_0$ e lo stato è finale.
 		% lez 13.2
		\begin{center}
			\begin{tikzpicture}[ SQUIGGLY/.style={->
				, decorate
				, decoration={snake,amplitude=.4mm,segment length=2mm,post length=1mm}},
				]
				\begin{scope}[local bounding box=wordScope, start chain=word, node distance=0pt]
					\node [draw, minimum width=65pt, minimum height=25pt, on chain=word] {$\dots$};
					\node [minimum width=25pt, minimum height=25pt, on chain=word] {};
				\end{scope}

				\node[draw, minimum width=25pt, minimum height=25pt] (state) [below=of word-2] {$q_F$};
				\node[draw, minimum width=25pt, minimum height=25pt] (stack) [right=of state] {$Z_0$};

				\draw[SQUIGGLY] (state.north) to (word-2.south);
				\draw[SQUIGGLY] (state.east) to (stack.west);
			\end{tikzpicture}
		\end{center}
 	\item le mosse sulla pila possono essere solo
 		\begin{itemize}
 			\item push di un simbolo
 			\item pop di un simbolo
		 	\item pila invariata
 		\end{itemize}
 		quindi il pop non è più implicito
 	\item se una mossa legge un simbolo da input, allora non modifica la pila. 
 		Cioè le mosse che manipolano la pila sono seperate da quelle che manipolano l'input.
\end{itemize}
In questa forma
$$ \delta : Q \times (\Sigma \cup \{\varepsilon\}) \times \Gamma \rightarrow 2^{Q \times \{-, \text{pop}, a \in \Gamma \mid \text{push}(A)\}} $$
ed abbiamo che le mosse possono avere le seguenti forme
\begin{itemize}
 	\item mosse di lettura: $ (p, -) \in \delta(q, a, A) \hspace{1cm} a \in \Sigma \cup \{\varepsilon\} $
 	\item pop: $(p, \text{pop}) \in \delta(q, \varepsilon , A) $
 	\item push: $(p, \text{push}(B)) \in \delta(q, \varepsilon , A) $
 	\item mosse che lasciano la pila invariata: $(p, -) \in \delta(q, \varepsilon , A) $
\end{itemize}
 
\begin{tcolorbox}
 	Ad esempio se avessimo una sequenza di parentesi $([()]())$, la pila contiene inizialmente $Z_0$
	\begin{center}
		\begin{tikzpicture}[ SQUIGGLY/.style={->
			, decorate
			, decoration={snake,amplitude=.4mm,segment length=2mm,post length=1mm}},
			]
			\begin{scope}[local bounding box=col1, start chain=col1 going above, node distance=0pt]
				\node [minimum width=25pt, minimum height=25pt, on chain=col1] {$Z_0$};
			\end{scope}
			\begin{scope}[local bounding box=col2, start chain=col2 going above, node distance=0pt]
				\node [minimum width=25pt, minimum height=25pt, on chain=col2] [right=of col1-1]{$Z_0$};
				\node [minimum width=25pt, minimum height=25pt, on chain=col2] {$($};
			\end{scope}
			\begin{scope}[local bounding box=col3, start chain=col3 going above, node distance=0pt]
				\node [minimum width=25pt, minimum height=25pt, on chain=col3] [right=of col2-1]{$Z_0$};
				\node [minimum width=25pt, minimum height=25pt, on chain=col3] {$($};
				\node [minimum width=25pt, minimum height=25pt, on chain=col3] {$[$};
			\end{scope}
			\begin{scope}[local bounding box=col4, start chain=col4 going above, node distance=0pt]
				\node [minimum width=25pt, minimum height=25pt, on chain=col4] [right=of col3-1]{$Z_0$};
				\node [minimum width=25pt, minimum height=25pt, on chain=col4] {$($};
				\node [minimum width=25pt, minimum height=25pt, on chain=col4] {$[$};
				\node [minimum width=25pt, minimum height=25pt, on chain=col4] {$($};
			\end{scope}
			\begin{scope}[local bounding box=col5, start chain=col5 going above, node distance=0pt]
				\node [minimum width=25pt, minimum height=25pt, on chain=col5] [right=of col4-1]{$Z_0$};
				\node [minimum width=25pt, minimum height=25pt, on chain=col5] {$($};
				\node [minimum width=25pt, minimum height=25pt, on chain=col5] {$[$};
			\end{scope}
			\begin{scope}[local bounding box=col6, start chain=col6 going above, node distance=0pt]
				\node [minimum width=25pt, minimum height=25pt, on chain=col6] [right=of col5-1]{$Z_0$};
				\node [minimum width=25pt, minimum height=25pt, on chain=col6] {$($};
			\end{scope}
			\begin{scope}[local bounding box=col7, start chain=col7 going above, node distance=0pt]
				\node [minimum width=25pt, minimum height=25pt, on chain=col7] [right=of col6-1]{$Z_0$};
				\node [minimum width=25pt, minimum height=25pt, on chain=col7] {$($};
				\node [minimum width=25pt, minimum height=25pt, on chain=col7] {$($};
			\end{scope}
			\begin{scope}[local bounding box=col8, start chain=col8 going above, node distance=0pt]
				\node [minimum width=25pt, minimum height=25pt, on chain=col8] [right=of col7-1]{$Z_0$};
				\node [minimum width=25pt, minimum height=25pt, on chain=col8] {$($};
			\end{scope}
			\begin{scope}[local bounding box=col9, start chain=col9 going above, node distance=0pt]
				\node [minimum width=25pt, minimum height=25pt, on chain=col9] [right=of col8-1]{$Z_0$};
			\end{scope}

			% \node [below=of col2-1] {$($};
			% \node [below=of col3-1] {$[$};
			% \node [below=of col4-1] {$($};
			% \node [below=of col5-1] {$)$};
			% \node [below=of col6-1] {$]$};
			% \node [below=of col7-1] {$($};
			% \node [below=of col8-1] {$)$};
			% \node [below=of col9-1] {$)$};

			\node [left=of col1-1, xshift=1cm, yshift=2.5cm] {\rotatebox[origin=c]{90}{\tiny Pila}};
			\node [below=of col9-1, yshift=1cm] {\tiny Input};

			\draw[->] ([xshift=-0.45cm] col1-1.south) -- ([xshift=8cm] col1-1.south);
			\draw[->] ([yshift=-0.45cm] col1-1.west) -- ([yshift=3cm] col1-1.west);
			\draw[dashed] ([xshift=-2.75cm] col4-2.south) -- ([xshift=4.5cm] col4-2.south) node[xshift=0.5cm] {\tiny $([()]())$};
			\draw[dashed] ([xshift=-2.75cm] col4-3.south) -- ([xshift=4.5cm] col4-3.south) node[xshift=0.5cm] {\tiny $[()], ()$};
			\draw[dashed] ([xshift=-2.75cm] col4-4.south) -- ([xshift=4.5cm] col4-4.south) node[xshift=0.5cm] {\tiny $()$};

		\end{tikzpicture}
	\end{center}
 	questo disegno mostra la natura ricorsiva degli automi.
	Infatti visto che la pila di questo automa non può mai scendere sotto il suo livello iniziale, tutte le evoluzioni definite dalle linee tratteggiate definiscono parole valide del linguaggio.
\end{tcolorbox}
 
\begin{nota}
	Gli automi che abbiamo visto fino ad ora possono essere simulati da questa versione normalizzata, scomponendo una mossa una pop ed una serie di push utilizzando degli stati ausiliari.
\end{nota}

\begin{proof}[Da un automa a pila costruiamo una grammatica di tipo 2]
	% todo sistemo bene
\end{proof}

% lezione 14
Ripetiamo la versione di automa a pila semplificato vista a lezione scorsa, in questo per riuscire ad accettare dobbiamo arrivare in uno stato finale con solo $Z_0$ lo stato finale sulla pila.

Dobbiamo trovare un modo di trasformare un automa a pila come definito nella lezione scorsa, in una grammatica.
Questa grammatica ha nonterminali della forma $[qAp]$ con $q, p \in Q$ e $A \in V$, e rappresenta:
\begin{itemize}
	\item $q$ è lo stato in cui si inizia
	\item $p$ è lo stato in cui si finisce 
	\item e $A$ è il simbolo in cima alla pila all'inizio e alla fine della computazione.
\end{itemize}
% fig 14.1
\begin{figure}[H]
	\centering
	\begin{tikzpicture}
		\draw[->] (0, 0) -- (0, 4) node[left, yshift=-0.2cm] {\rotatebox{90}{\tiny pila}};
		\draw[->] (0, 0) -- (5, 0) node[below, xshift=-0.2cm] {\tiny input};

		\node[label=left:{\tiny $q, A$}] at (1, 1) [circle,fill,inner sep=1.5pt] (a) {};
		\node[label=right:{\tiny $p, A$}] at (4, 1) [circle,fill,inner sep=1.5pt] (b) {};

		\tikzmath {
			real \x, \rand, \precx, \precy;
			\precx = 1;
			\precy = 1;
			for \x in {1.2,1.4,...,3.8} {
				\rand = 1 + (random(0, 100) / 66);
				{ \draw[-] (\precx, \precy) -- (\x, \rand); };
				\precx = \x;
				\precy = \rand;
			};
			{ \draw[-] (\precx, \precy) -- (4, 1); };
		}
		\draw[dashed] (1, 1) -- (1, 0);
		\draw[dashed] (4, 1) -- (4, 0);
		\draw[dashed] (1, 1) -- (4, 1);
		\draw[decorate, decoration={brace, mirror, amplitude=0.2cm}] (1, 0) to node[below, yshift=-0.2cm] {\tiny $w$} (4, 0);
	\end{tikzpicture}
	\caption{La computazione rappresentata dal simbolo $[qAp]$}
\end{figure}
Infatti negli automi come li abbiamo definiti, vale la proprietà per cui ??? % Z_0

Definiamo ora le regole di produzione della grammatica induttivamente come le stringhe riconosciute dalla computazione $[qAp]$:
\begin{itemize}
	\item base: abbiamo due casi
		\begin{itemize}
			\item caso $0$: il caso più semplice è $[qAq]$, qui l'unica parola riconosciuta è $\varepsilon$, quindi è necessaria la regola
				$$ [qAq] \rightarrow \varepsilon $$
				Quindi creo tutte le produzioni della forma
				$$ \forall q \in Q, A \in \Gamma \mid [qAq] \rightarrow \varepsilon $$	% domanda pighi su computazione che inizia e finisce nello stato q
			\item caso $0'$: il secondo caso più semplice è quello in qui si è nello stato $q$, si consuma un carattere o nessuno, e questo ci porta nello stato $p$; cioè $(p, -) \in \delta(q, a, A)$ con $a \in \Sigma \cup \{\varepsilon\}$.
				Questo si traduce nella produzione
				$$ [qAp] \rightarrow a, \hspace{1cm} a \in \Sigma \cup \{\varepsilon\} $$
				Quindi creo tutte le produzioni della forma
				$$ \forall q, p \in Q, A \in \Gamma, a \in \Sigma \cup \{\varepsilon\} \mid [qAp] \rightarrow a $$
		\end{itemize}
	\item passo: si distinguono due casi
		\begin{figure}[H]
			\centering
			\begin{subfigure}{0.45\textwidth}
				\centering
				\begin{tikzpicture}
					\draw[->] (0, 0) -- (0, 4) node[left, yshift=-0.2cm] {\rotatebox{90}{\tiny pila}};
					\draw[->] (0, 0) -- (5, 0) node[below, xshift=-0.2cm] {\tiny input};

					\node[label=left:{\tiny $q, A$}] at (1, 1) [circle,fill,inner sep=1.5pt] (a) {};
					\node[label=right:{\tiny $p, A$}] at (4, 1) [circle,fill,inner sep=1.5pt] (b) {};
					\node[label=below right:{\tiny $r, A$}] at (3, 1) [circle,fill,inner sep=1.5pt] (c) {};
			
					\tikzmath {
						real \x, \rand, \precx, \precy;
						\precx = 1;
						\precy = 1;
						for \x in {1.2,1.4,...,2.8} {
							\rand = 1.2 + (random(0, 100) / 80);
							{ \draw[-] (\precx, \precy) -- (\x, \rand); };
							\precx = \x;
							\precy = \rand;
						};
						{ \draw[-] (\precx, \precy) -- (3, 1); };
						\precx = 3;
						\precy = 1;
						for \x in {3.2,3.4,...,3.8} {
							\rand = 1.2 + (random(0, 100) / 80);
							{ \draw[-] (\precx, \precy) -- (\x, \rand); };
							\precx = \x;
							\precy = \rand;
						};
						{ \draw[-] (\precx, \precy) -- (4, 1); };
					}
					\draw[dashed] (1, 1) -- (1, 0);
					\draw[dashed] (3, 1) -- (3, 0);
					\draw[dashed] (4, 1) -- (4, 0);
					\draw[dashed] (1, 1) -- (4, 1);
					\draw[decorate, decoration={brace, mirror, amplitude=0.2cm}] (1, 0) to node[below, yshift=-0.1cm] {\tiny $w'$} (3, 0);
					\draw[decorate, decoration={brace, mirror, amplitude=0.2cm}] (3, 0) to node[below, yshift=-0.1cm] {\tiny $w''$} (4, 0);
					\draw[decorate, decoration={brace, mirror, amplitude=0.2cm}] (1, -0.5) to node[below, yshift=-0.1cm] {\tiny $w$} (4, -0.5);
				\end{tikzpicture}
				\caption{Caso 2}
			\end{subfigure}
			\begin{subfigure}{0.45\textwidth}
				\centering
				\begin{tikzpicture}
					\draw[->] (0, 0) -- (0, 4) node[left, yshift=-0.2cm] {\rotatebox{90}{\tiny pila}};
					\draw[->] (0, 0) -- (5, 0) node[below, xshift=-0.2cm] {\tiny input};

					\node[label=left:{\tiny $q, A$}] at (1, 1) [circle,fill,inner sep=1.5pt] (a) {};
					\node[label=left:{\tiny $q', B$}] at (1.1, 1.4) [circle,fill,inner sep=1.5pt] (b) {};
					\node[label=right:{\tiny $p', B$}] at (3.9, 1.4) [circle,fill,inner sep=1.5pt] (c) {};
					\node[label=right:{\tiny $p, A$}] at (4, 1) [circle,fill,inner sep=1.5pt] (d) {};

					\draw[-] (a) -- (b);
					\draw[-] (c) -- (d);
			
					\tikzmath {
						real \x, \rand, \precx, \precy;
						\precx = 1.1;
						\precy = 1.4;
						for \x in {1.2,1.4,...,3.6} {
							\rand = 1.5 + (random(0, 100) / 80);
							{ \draw[-] (\precx, \precy) -- (\x, \rand); };
							\precx = \x;
							\precy = \rand;
						};
						{ \draw[-] (\precx, \precy) -- (3.9, 1.4); };
					}
					\draw[dashed] (1, 1) -- (1, 0);
					\draw[dashed] (4, 1) -- (4, 0);
					\draw[dashed] (1, 1) -- (4, 1);
					\draw[dashed] (1.1, 1.4) -- (3.9, 1.4);
					\draw[decorate, decoration={brace, mirror, amplitude=0.2cm}] (1, 0) to node[below, yshift=-0.2cm] {\tiny $w$} (4, 0);
				\end{tikzpicture}
				\caption{Caso 1}
			\end{subfigure}
		\end{figure}
		% fig 14.2
		\begin{itemize}
			\item caso 1: nei passi intermedi (tranne l'ultimo) la pila è sempre strettamente più alta di quando si è iniziato, questo si traduce in
				$$ [qAp] \rightarrow [q'Bp'] $$
				con $(q', \text{push}(B)) \in \delta(q, \varepsilon, A)$ e $(p, \text{pop}) \in \delta(p', \varepsilon, B)$.
				Quindi 
				\begin{multline*}
				\forall q, q', p, p' \in Q, A, B \in \Gamma \\ \mid
					(q', \text{push}(B)) \in \delta(q, \varepsilon, A) \wedge (p, \text{pop}) \in \delta(p', \varepsilon, B) \\
					\Rightarrow [qAp] \rightarrow [q'Bp'] 
				\end{multline*}
			\item caso 2: la computazione $[qAp]$ svuota la pila fino alla $A$ inizia e poi continua, allora possiamo scomporre la computazione in due parti, quindi
				$$ \forall q, p, r \in Q, A \in \Gamma \mid [qAp] \rightarrow [qAr][rAp] $$
		\end{itemize}
\end{itemize}
Si può dimostrare che
\begin{lemma}
	$\forall q, p \in Q, A \in \Gamma, w \in \Sigma^* \mid [qAp] \overset{*}{\Rightarrow} w$ sse 
	% fig 14.3
	l'automa $M$ in una configurazione con $A$ in cima alla pila, stato $q$, dopo aver letto $w$ raggiunge una configurazione in cui il contentuto della pila è lo stesso dell'inizio, lo stato è $p$ e nei passi intermedi la pila non scende mai sotto il livello iniziale.
\end{lemma}
Quindi durante una computazione quello che c'è sotto al simbolo in cima alla pina all'inizio della computazione non è rilevante.

Per finire di costruire la grammatica manca di definire l'assioma.
Prima di tutto si può notare che visto che l'automa parte nello stato $q_0$ con $Z_0$ e basta sulla pila, una stringa $w$ può essere generata solo dalle triple 
$$[q_0Z_0q_F] \overset{*}{\Rightarrow} w$$
con $q_0$ iniziale e $q_F$ finale.
Quindi definiamo l'insieme dei nonterminali della grammatica $V$ come l'insieme di tutte le triple definite induttivamente sopra unito ad un nuovo nonterminale $S$ tale che
$$ \forall q_F \in F \mid S \rightarrow [q_0 Z_0 q_F] \in P $$
e questo $S$ così definito è il simbolo iniziale.

\section{Forme normali per le grammatiche di tipo 2}
Si può vedere che tutte le produzioni generate dalla traduzione da automa a pila a grammatica sono di pochi tipi: variabile a terminale, variabile a variabile e variabile a coppia di variabili.
Da questo fatto e dal fatto che i linguaggi riconosciuti dagli automi a pila sono esattamente quelli generati dalle grammatiche di tipo 2, ci rendiamo conto che possiamo restringere di molto il tipo di forma che il lato destro di una produzione di una grammatica CF può assumere.
Nel caso di sopra appunto da variabile a terminale, da variabile a variabile e da variabile a coppia di variabili.

Vediamo ora due forme normali.
Ogni grammatica può essere trasformata in una di queste forme normali a patto di sacrificare la parola vuota.

\subsection{Forma normale di Greibach}
In una grammatica in FNG (Forma Normale di Greibach) tutte le produzioi sono della forma
$$ A \rightarrow a B_1 \dots B_k, \hspace{1cm} a \in \Sigma, A, B_1, \dots, B_k \in V, k \geq 0 $$

Supponiamo di avere la gramamtica
\begin{align*}
	A &\rightarrow a B B \\
	A &\rightarrow b \\
	B &\rightarrow b B \\
	B &\rightarrow b
\end{align*}
e di aver fatto la trasformazione in automa a pila.
In questa forma normale la pila avrà in cima sempre un terminale e quindi si può avere un simbolo di lookahead e scegliere più precisamente la prossima produzione da utilizzare, anche se non si toglie il nondeterminismo (v. $B \rightarrow b B$ e $B \rightarrow b$).
Un altro vantaggio di avere sempre un terminale in cima alla pila è che in questo tipo di automa si possono eliminare le $\varepsilon$-mosse.

\subsection{Forma normale di Chomsky}
Nella FNC (Forma Normale di Chomsky) ci sono solo due tipi di regole
\begin{align*}
	A & \rightarrow B C & A, B, C \in V \\
	A & \rightarrow a & A \in V, a \in \Sigma \\
\end{align*}
Questa genera alberi di derivazione binari, salvo sulle foglie; ed è comoda per studiare alcune proprietà combinatorie.

\subsubsection{Trasformazione in FNC}
Data una grammatica genererica $G$ eseguiamo i seguenti passi (l'ordine è importante) per trasformarla in FNC:
\begin{enumerate}
	\item eliminazione delle $\varepsilon$-produzioni: diciamo che una variabile $A$ è \textit{cancellabile} sse $A \overset{*}{\Rightarrow} \varepsilon$.
		Induttivamente $A$ è cancellabile se
		\begin{itemize}
			\item banalmente $A \rightarrow \varepsilon$
			\item o se $A \rightarrow X_1 X_2 \dots X_k$ e $X_1, X_2, \dots, X_k$ sono tutti cancellabili.
		\end{itemize}
		Questo può essere definito come una chiusura dove
		$$ C_0 = \{ A \mid A \rightarrow \varepsilon \} $$
		e 
		$$ C_i = C_{i - 1} \cup \{ A \mid \exists A \rightarrow X_1 X_2 \dots X_k \; \text{con} \; \forall i \in 1, \dots, k \mid X_i \in C_{i - 1} \} $$
		Visto che
		$$ C_0 \subseteq C_1 \subseteq \dots \subseteq V $$
		e $V$ è finito, allora esiste un $i$ tale che $C_i = C_{i - 1}$.

		Ora sia $C$ l'insisme delle variabili cancellabili, costruiamo una grammatica $G' = \langle V, \Sigma, P', S \rangle$ con $P'$ costituito da tutte le produzioni di $P$ eccetto le $\varepsilon$ produzioni e per ogni produzione $A \rightarrow X_1 X_2 \dots X_k$ con $X_1, X_2, \dots, X_k \in V \cup \Sigma$ aggiungo a $P'$ le produzioni $A \rightarrow X_{i_1} X_{i_2} \dots X_{i_j}$ tali che $1 \leq i_1 < i_2 < \dots < i_j \leq k$ e per $\forall X_l \not \in X_{i_1}, \dots, X_{i_j} \mid X_l \in C$ e $j \geq 1$.

		\begin{tcolorbox}
			Supponiamo di avere nella gramamtica $G$ che vogliamo trasformare la produzione
			$$ A \rightarrow B C a D $$
			e che l'insieme delle variabili cancellabili è $C = \{C, D\}$.

			Nella mia gramamtica $G'$ simulo la cancellazione di $C$ e $D$ aggiungendo le produzioni
			\begin{align*}
				A &\rightarrow B a D \\
				A &\rightarrow B C a \\
				A &\rightarrow B a \\
			\end{align*}
		\end{tcolorbox}

		\begin{tcolorbox}
			Supponiamo di avere nella gramamtica $G$ che vogliamo trasformare la produzone
			$$ A \rightarrow C D E $$
			e che l'insieme delle variabili cancellabili è $C = \{C, D, E\}$.

			Nella mia gramamtica $G'$ simulo la cancellazione di $C$ e $D$, quindi aggiungo le produzioni
			\begin{align*}
				A &\rightarrow C D \\
				A &\rightarrow C E \\
				A &\rightarrow D E \\
				A &\rightarrow C \\
				A &\rightarrow D \\
				A &\rightarrow E \\
			\end{align*}
		\end{tcolorbox}

		Visto che le produzioni da aggiungere sotto tutti i sottoinsiemi delle variabili cancellabili di un lato destro meno l'insieme vuoto, vengono aggiunge nel caso peggiore un numero esponenziale di produzioni.
	\item eliminazione delle produzioni unitarie: una produzione unitaria è una produzione della forma
		$$ A \rightarrow B, \hspace{1cm} A, B \in V $$
		Costruiamo similmente a prima l'insieme di tutte le coppie di variabili $X, Y$ tali per cui $X \overset{+}{\Rightarrow} Y$, cioè per cui vale
		Abbiamo quindi
		$$ X \rightarrow A_1 \rightarrow \dots \rightarrow Y $$
		questo processo infatti le catene sono di lunghezza al più $|V|$ senza contenere cicli.

		Nella nuova grammatica tolgo tutte le produzioni unitarie e se $X \rightarrow \dots \rightarrow Y \rightarrow \alpha$ e $\alpha \in \Sigma$ oppure $|\alpha| > 1$, allora aggiungo la produzione $X \rightarrow \alpha$.
	\item eliminazione simboli inutili: $X \in V \cup \Sigma$ è utile sse $\exists S \overset{*}{\Rightarrow} \alpha X \beta \overset{*}{\Rightarrow} w \in \Sigma^*$.
		Questi sono eliminati utilizzando algoritmi sui grafi (chiusura bottom up, chiusura top down, non lo ha spiegato ma ci sono negli appunti del Santini).
	\item eliminazione dei terminali: in tutte le produzioni $A \rightarrow \alpha$ con $|\alpha| > 1$ si introducono nonterminali per ogni terminale.
		\begin{tcolorbox}
			Supponiamo di avere le produzioni
			\begin{align*}
				A &\rightarrow A aab C \\
				A &\rightarrow b C \\
				A &\rightarrow b b \\
			\end{align*}
			introduciamo i nonterminali $X_a$ e $X_b$ e le regole
			\begin{align*}
				A &\rightarrow A X_a X_a X_b C \\
				A &\rightarrow X_b C \\
				A &\rightarrow X_b X_b \\
				X_a & \rightarrow a \\
				X_b & \rightarrow b
			\end{align*}
		\end{tcolorbox}
	\item binarizzazione delle produzioni: per ogni produzione $A \rightarrow B_1 B_2 \dots B_k$ con $k > 2$, si introducono delle produzioni intermedie
		\begin{align*}
			A &\rightarrow B_1 Z_1 \\
			Z_1 &\rightarrow B_2 Z_2 \\
			    &\vdots \\
			Z_{k - 2} &\rightarrow B_{k - 1} B_k
		\end{align*}
\end{enumerate}
\begin{tcolorbox}[breakable]
	Date le produzioni
	\begin{align*}
		S &\rightarrow a B \\
		S &\rightarrow b A \\
		A &\rightarrow a \\
		A &\rightarrow a S \\
		A &\rightarrow b A A \\
		B &\rightarrow b \\
		B &\rightarrow b S \\
		B &\rightarrow a B B \\
	\end{align*}
	questa è già priva di $\varepsilon$-produzioni, produzioni unitarie e tutti i simboli sono utili.

	Ora eliminiamo i terminali e otteniamo
	\begin{align*}
		S &\rightarrow X_a B \\
		S &\rightarrow X_b A \\
		A &\rightarrow a \\
		A &\rightarrow X_a S \\
		A &\rightarrow X_b A A \\
		B &\rightarrow b \\
		B &\rightarrow X_b S \\
		B &\rightarrow X_a B B \\
		X_a &\rightarrow a \\
		X_b &\rightarrow b \\
	\end{align*}
	ed ora binarizziamo le produzioni
	\begin{align*}
		S &\rightarrow X_a B \\
		S &\rightarrow X_b A \\
		A &\rightarrow a \\
		A &\rightarrow X_a S \\
		A &\rightarrow X_b E_1 \\
		E &\rightarrow A A \\
		B &\rightarrow b \\
		B &\rightarrow X_b S \\
		B &\rightarrow X_a E_2 \\
		E_2 &\rightarrow B B \\
		X_a &\rightarrow a \\
		X_b &\rightarrow b \\
	\end{align*}
\end{tcolorbox}

% lezione 15
% magari chapter invece che section
\section{Appartenenza ai Context Free di un linguaggio}
Dato un linguaggio ci possiamo chiedere se questo sia CF.
Ad esempio
$$ L = \{ a^l b^k c^j \mid k = j \} \overset{?}{\in} \text{CF} $$
Un linguaggio è CF se possiamo costruire una grammatica di tipo 2 o un automa a pila.
Ad esempio per il linguaggio di sopra possiamo consumare tutte le $a$ e controllare che il numero di $b$ e di $c$ sia uguale con una pila.

Prendiamo invece
$$ L = \{ a^i b^k c^j \mid i = j = j \} \overset{?}{\in} \text{CF} $$
L'automa per il linguaggio di prima non può essere adattato a questo linguaggio.

\begin{tcolorbox}[breakable]
	Prendiamo la grammatica
	\begin{align*}
		S &\rightarrow [S] \mid S S \mid T \\
		T &\rightarrow (T) \mid T T \mid \varepsilon
	\end{align*}
	e una derivazione
	$$ S \overset{*}{\Rightarrow} [()[]] $$
	Ora un albero di derivazione che possiamo fare per questa stringa è
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[scale=1, every node/.style={transform shape}]
			\node (root) {$S$}
				[sibling distance=1cm]
				child { node {$[$} }
					child { node {$S$} 
						[sibling distance=3cm]
						child { node {$S$} 
							child { node {$T$} 
								[sibling distance=1cm]
								child { node {$($} }
								child { node {$T$} 
									child { node {$\varepsilon$} }
								}
								child { node {$)$} }
							}
						}
						child { node {$S$} 
							[sibling distance=1cm]
							child { node {$[$} }
							child { node {$S$} 
								child { node {$T$}
									child { node {$\varepsilon$} }
								}
							}
							child { node {$]$} }
						}
					}
				child { node {$]$} };

			\node[draw, fit=(root-2-1-1) (root-2-1-1-1) (root-2-1-1-2) (root-2-1-1-3) (root-2-1-1-2-1)] (bound1) {};
			\node[draw, fit=(root-2-2-2-1) (root-2-2-2-1-1)] (bound2) {};

			\node[left] at (bound1.west) {$T \overset{*}{\Rightarrow} ()$};
			\node[right] at (bound2.east) {$T \overset{*}{\Rightarrow} \varepsilon$};
		\end{tikzpicture}
	\end{figure}
	% fig 15.1
	sugli alberi di derivazione si possono fare operazione di sostituzione di sottoalberi, ad esempio nell'albero di sopra sostituenzo il sottoalbero 2 con il sottoalbero 1 otteniamo l'albero di derivazione per $[()[()]]$.
	In generale quello di sopra è un particolare albero che è rappresentato dalla derivazione
	$$ A \overset{*}{\Rightarrow} v A x, \hspace{1cm} v, x \in \Sigma^* $$
	questi sono interessanti perché possiamo ???.
	Ad esempio nell'albero prima abbiamo
	$$ S \overset{*}{\Rightarrow} [() S ] $$
	possiamo vedere che possiamo sia accorciare la derivazione
	% fig 15.2
	\begin{figure}[H]
		\centering
		\begin{subfigure}{\textwidth}
			\centering
			\begin{tikzpicture}[scale=1, every node/.style={transform shape}]
				\node (root) {$S$}
					[sibling distance=1cm]
					child { node {$[$} }
						child { node {$S$} 
							[sibling distance=3cm]
							child { node {$S$} 
								child { node {$T$} 
									[sibling distance=1cm]
									child { node {$($} }
									child { node {$T$} 
										child { node {$\varepsilon$} }
									}
									child { node {$)$} }
								}
							}
							child { node {$S$} 
								[sibling distance=1cm]
								child { node {$[$} }
								child { node {$S$} 
									child { node {$T$}
										child { node {$\varepsilon$} }
									}
								}
								child { node {$]$} }
							}
						}
					child { node {$]$} };

				\node[draw, fit=(root-2-2) (root-2-2-1) (root-2-2-2) (root-2-2-3) (root-2-2-2-1) (root-2-2-2-1-1)] (bound) {};
			\end{tikzpicture}
		\end{subfigure}
		\begin{subfigure}{0.45\textwidth}
			\centering
			\begin{tikzpicture}[scale=0.5, every node/.style={transform shape}]
				\node (root) {$S$}
					[sibling distance=1cm]
					child { node {$[$} }
						child { node {$S$} 
							[sibling distance=3cm]
							child { node {$S$} 
								child { node {$\dots$} }
							}
							child { node {$S$} 
								[sibling distance=1cm]
								child { node {$[$} }
								child { node {$S$} 
									child { node {$[$} }
									child { node {$S$} 
										child { node {$[$} }
										child { node {$S$} 
											child { node {$T$}
												child { node {$\varepsilon$} }
											}
										}
										child { node {$]$} }
									}
									child { node {$]$} }
								}
								child { node {$]$} }
							}
						}
					child { node {$]$} };
			\end{tikzpicture}
			\caption{Allungamento ($uv^2wx^2z$)}
		\end{subfigure}
		\begin{subfigure}{0.45\textwidth}
			\centering
			\begin{tikzpicture}[scale=0.5, every node/.style={transform shape}]
				\node (root) {$S$}
					[sibling distance=1cm]
					child { node {$[$} }
						child { node {$S$} 
							[sibling distance=3cm]
							child { node {$S$} 
								child { node {$\dots$} }
							}
							child { node {$S$} 
								[sibling distance=1cm]
								child { node {$T$}
									child { node {$\varepsilon$} }
								}
							}
						}
					child { node {$]$} };
			\end{tikzpicture}
			\caption{Accorciamento ($uwz$)}
		\end{subfigure}
	\end{figure}
	In questo modo possiamo generare infinite stringhe.
\end{tcolorbox}

\subsection{Pumping lemma}% TODO: posiziono bene 

Dal fatto che le grammatiche possono essere convertite in FNC (a patto di sacrificare la parola vuota), lavoreremo con grammatiche in FNC per semplificare la dimostrazione del pumping lemma.

Definiamo la \textit{profondità} (o altezza) di un albero come il più lungo cammino dalla radice ad una foglia.
\begin{lemma}
	Sia
	$$ G = \langle V, \Sigma, P, S \rangle $$
	una grammatica in FNC e sia $T : A \overset{*}{\Rightarrow} w \in \Sigma^*$ un albero di derivazione di altezza $h$.
	Allora la lunghezza di $w$ è minore o uguale a $2^{h - 1}$.
	$$ |w| \leq 2^{h - 1} $$
\end{lemma}
\begin{proof}
	Procediamo per induzione su $h$:
	\begin{itemize}
		\item per $h = 1$: per forza l'albero deve rappresentare una produzione della forma $A \rightarrow a \in \Sigma$, quindi $ w = a $ e 
			$$ |w| = 1 = 2^0 = 2^{1 - 1} $$
		\item la produzione applicata alla radice deve per forza essere della forma $A \rightarrow B C$, quindi l'albero si divide in due sottoalberi, un albero $T' : B \overset{*}{\Rightarrow} w'$ e un albero $T'' : C \overset{*}{\Rightarrow} w''$.
			\begin{center}
				\begin{tikzpicture}[
						PUNTO/.style={minimum size=0, inner sep=0, outer sep=0},
					]
					\node[PUNTO, label=above:{$A$}] at (0, 0) (top) {};

					\node[minimum size=5pt, inner sep=0, outer sep=0, circle, fill] at (2, -1) (top-right) {};
					\node[minimum size=5pt, inner sep=0, outer sep=0, circle, fill] at (-2, -1) (top-left) {};

					\node at (0, -1) {$T$};
					\node at (-2, -3) {$T'$};
					\node at (2, -3) {$T''$};


					\node[PUNTO] at (-4, -4) (bottom-left) {};
					\node[PUNTO] at (4, -4) (bottom-right) {};
					\node[PUNTO] at (0, -4) (bottom-center) {};

					\draw[-] (top) -- (top-left); 
					\draw[-] (top) -- (top-right); 
					\draw[-] (bottom-left) -- (bottom-right); 

					\draw[-] (top-left) -- (bottom-center);
					\draw[-] (top-right) -- (bottom-center);
					\draw[-] (top-right) -- (bottom-right);
					\draw[-] (top-left) -- (bottom-left);

					\draw[<->] ([xshift=-5pt] -4, -4) -- node[left] {$h - 1$} ([xshift=-5pt] -4, -1);
					\draw[<->] ([xshift=5pt] 4, -4) -- node[right] {$h$} ([xshift=5pt] 4, 0);

					\draw[dashed, -] (-4, -1) -- (top-left);
					\draw[dashed, -] (4, 0) -- (top);

					\draw[decorate, decoration={brace, mirror, amplitude=0.2cm}] ([yshift=-5pt] bottom-left.south) to node[below, yshift=-0.2cm] {\tiny $w'$} ([yshift=-5pt] bottom-center.south);
					\draw[decorate, decoration={brace, mirror, amplitude=0.2cm}] ([yshift=-5pt] bottom-center.south) to node[below, yshift=-0.2cm] {\tiny $w''$} ([yshift=-5pt] bottom-right.south);
					\draw[decorate, decoration={brace, mirror, amplitude=0.2cm}] ([yshift=-20pt] bottom-left.south) to node[below, yshift=-0.2cm] {\tiny $w$} ([yshift=-20pt] bottom-right.south);
				\end{tikzpicture}
			\end{center}
			% fig 15.3
			Questi due hanno altezza minore o uguale ad $h - 1$.
			Ora applicando l'ipotesi induttiva 
			\begin{align*}
				|w'| &\leq 2^{h - 2} \\
				|w''| &\leq 2^{h - 2}
			\end{align*}
			e 
			$$ |w| = |w'| + |w''| \leq 2^{h - 2} + 2^{h - 2} = 2^{h - 1} $$
	\end{itemize}
\end{proof}

\begin{lemma}[Pumping lemma per CFL]
	Sia $L$ un linguagio CF allora $\exists N > 0$ tale che $\forall z \in L$ con $|z| \geq N$, questa può essere scomposta
	$$ z = uvwxy $$
	tali che
	\begin{enumerate}
		\item $|vwx| \leq N $
		\item $vx \neq \varepsilon $
		\item $\forall i \geq 0 \mid uv^i w x^i y \in L$
	\end{enumerate}
\end{lemma}
\begin{proof}
	Sia $G = \langle V, \Sigma, P, S \rangle$ una grammatica in FNC per $L \setminus \{ \varepsilon \}$.
	Sia $k = | V | $ e definiamo $N = 2^k$.

	Sia $z \in L$ con $|z| \geq N$, allora ha un albero di derivazione $T : S \overset{*}{\Rightarrow} z$
	\begin{center}
		\begin{tikzpicture}[
				PUNTO/.style={minimum size=0, inner sep=0, outer sep=0},
			]
			\node[PUNTO, label=above:{$S$}] at (0, 0) (top) {};

			\node[PUNTO] at (2, -4) (top-right) {};
			\node[PUNTO] at (-2, -4) (top-left) {};

			\draw[-] (top) -- (top-left); 
			\draw[-] (top) -- (top-right); 
			\draw[-] (top-left) -- (top-right); 

			\draw[<->] ([xshift=5pt] 3, -4) -- node[right] {$\geq k + 1$} ([xshift=5pt] 3, 0);
			\draw[decorate, decoration={brace, mirror, amplitude=0.2cm}] ([yshift=-5pt] top-left.south) to node[below, yshift=-0.2cm] {$z$} ([yshift=-5pt] top-right.south);
		\end{tikzpicture}
	\end{center}
	% fig 15.4
	visto che la lunghezza di $z$ è maggiore di $2^k$, allora dal lemma precedente abbiamo che l'altezza di $T$ è almeno $k + 1$, quindi esiste un cammino dalle foglie alle radici da $k + 1$ archi, quindi $k + 2$ nodi.
	Visto che l'ultimo nodo è un terminale, durante questo cammino incontreremo $k + 1$ non terminali, e quindi almeno un non terminale si ripeterà in questo percorso.
	Sia $A$ questo non terminale.
	\begin{figure}[H]
		\centering
		\begin{subfigure}{\textwidth}
			% sistemo label sotto pattern
			\centering
			\begin{tikzpicture}[
					PUNTO/.style={minimum size=0, inner sep=0, outer sep=0},
				]
				\node[circle, fill, minimum size=1pt, inner sep=1pt, label=above:{$S$}] at (0, 0) (top-1) {};
				\node[circle, fill, minimum size=1pt, inner sep=1pt, label=left:{$A$}] at (0, -2) (top-2) {};
				\node[circle, fill, minimum size=1pt, inner sep=1pt, label=right:{$A$}] at (0, -4) (top-3) {};

				\node[PUNTO] at (4, -6) (right-1) {};
				\node[PUNTO] at (2.66666, -6) (right-2) {};
				\node[PUNTO] at (1.33333, -6) (right-3) {};
				\node[PUNTO] at (-4, -6) (left-1) {};
				\node[PUNTO] at (-2.66666, -6) (left-2) {};
				\node[PUNTO] at (-1.33333, -6) (left-3) {};

				\path[name path=T21] (right-2) -- (top-2) -- (left-2);
				\path[name path=T22] (right-2) -- (left-2);
				\path[name path=T31] (right-3) -- (top-3) -- (left-3);
				\path[name path=T32] (right-3) -- (left-3);
				\tikzfillbetween[of=T21 and T22]{pattern={Lines[angle=45,distance={3pt/sqrt(2)}]},pattern color=gray, line width=0.2pt};
				\tikzfillbetween[of=T31 and T32]{white};
				\tikzfillbetween[of=T31 and T32]{pattern={crosshatch dots}, pattern color=gray, radius=0.1pt};

				\draw[-] (top-1) -- (left-1);
				\draw[-] (left-1) -- (right-1);
				\draw[-] (right-1) -- (top-1);
				\draw[-] (top-2) -- (left-2);
				\draw[-] (right-2) -- (top-2);
				\draw[-] (top-3) -- (left-3);
				\draw[-] (right-3) -- (top-3);


				\path[draw, -] (top-1) -- (-0.3, -0.75) -- (0.5, -1.25) -- (top-2) -- (-0.3, -2.75) -- (0.5, -3.25) -- (top-3);

				\draw[decorate, decoration={brace, mirror, amplitude=0.2cm}] ([yshift=-5pt] left-1.south) to node[below, yshift=-0.2cm] {\tiny $u$} ([yshift=-5pt] left-2.south);
				\draw[decorate, decoration={brace, mirror, amplitude=0.2cm}] ([yshift=-5pt] left-2.south) to node[below, yshift=-0.2cm] {\tiny $v$} ([yshift=-5pt] left-3.south);
				\draw[decorate, decoration={brace, mirror, amplitude=0.2cm}] ([yshift=-5pt] left-3.south) to node[below, yshift=-0.2cm] {\tiny $w$} ([yshift=-5pt] right-3.south);
				\draw[decorate, decoration={brace, mirror, amplitude=0.2cm}] ([yshift=-5pt] right-3.south) to node[below, yshift=-0.2cm] {\tiny $x$} ([yshift=-5pt] right-2.south);
				\draw[decorate, decoration={brace, mirror, amplitude=0.2cm}] ([yshift=-5pt] right-2.south) to node[below, yshift=-0.2cm] {\tiny $y$} ([yshift=-5pt] right-1.south);

				\draw[<->] ([xshift=5pt] 4, -6) -- node[right] {$\geq k + 1$} ([xshift=5pt] 4, 0);


			\end{tikzpicture}
		\end{subfigure}
		\begin{subfigure}{0.4\textwidth}
			\centering
			\begin{tikzpicture}[
					PUNTO/.style={minimum size=0, inner sep=0, outer sep=0}, 
					scale=0.45, 
					every node/.style={transform shape}
				]
				\node[PUNTO] at (0, 0) (top-1) {};
				\node[PUNTO] at (0, -2) (top-2) {};
				\node[PUNTO] at (0, -4) (top-3) {};
				\node[PUNTO] at (0, -6) (top-4) {};

				\node[PUNTO] at (4, -6) (right-1) {};
				\node[PUNTO] at (2.66666, -6) (right-2) {};
				\node[PUNTO] at (2.66666, -8) (right-3) {};
				\node[PUNTO] at (1.33333, -8) (right-4) {};

				\node[PUNTO] at (-4, -6) (left-1) {};
				\node[PUNTO] at (-2.66666, -6) (left-2) {};
				\node[PUNTO] at (-2.66666, -8) (left-3) {};
				\node[PUNTO] at (-1.33333, -8) (left-4) {};

				\path[name path=T21] (right-2) -- (top-2) -- (left-2);
				\path[name path=T22] (right-2) -- (left-2);

				\path[name path=T31] (right-3) -- (top-3) -- (left-3);
				\path[name path=T32] (right-3) -- (left-3);

				\path[name path=T41] (right-4) -- (top-4) -- (left-4);
				\path[name path=T42] (right-4) -- (left-4);

				\tikzfillbetween[of=T21 and T22]{pattern={Lines[angle=45,distance={3pt/sqrt(2)}]},pattern color=gray, line width=0.2pt};
				\tikzfillbetween[of=T31 and T32]{white};
				\tikzfillbetween[of=T31 and T32]{pattern={Lines[angle=45,distance={3pt/sqrt(2)}]}, pattern color=gray, line width=0.2pt};
				\tikzfillbetween[of=T41 and T42]{white};
				\tikzfillbetween[of=T41 and T42]{pattern={crosshatch dots}, pattern color=gray, radius=0.1pt};
				\draw[fill=black] (top-1) circle[radius=1pt];
				\draw[fill=black] (top-2) circle[radius=1pt]; 
				\draw[fill=black] (top-3) circle[radius=1pt];
				\draw[fill=black] (top-4) circle[radius=1pt];

				\node [above=of top-1, yshift=-25pt] {$S$};
				\node [above=of top-2, yshift=-25pt] {$A$};
				\node [above=of top-3, yshift=-25pt] {$A$};
				\node [above=of top-4, yshift=-25pt] {$A$};

				\draw[-] (top-1) -- (left-1);
				\draw[-] (right-1) -- (top-1);
				\draw[-] (top-2) -- (left-2);
				\draw[-] (right-2) -- (top-2);
				\draw[-] (top-3) -- (left-3);
				\draw[-] (right-3) -- (top-3);
				\draw[-] (right-3) -- (left-3);
				\draw[-] (top-4) -- (left-4);
				\draw[-] (top-4) -- (right-4);
				\draw[-] (left-1) -- (-1.33333, -6);
				\draw[-] (right-1) -- (1.33333, -6);
			\end{tikzpicture}
			\caption{Allungamento}
		\end{subfigure}
		\begin{subfigure}{0.4\textwidth}
			\centering
			\begin{tikzpicture}[
					PUNTO/.style={minimum size=0, inner sep=0, outer sep=0}, 
					scale=0.45, 
					every node/.style={transform shape}
				]
				\node[PUNTO] at (0, 0) (top-1) {};
				\node[PUNTO] at (0, -2) (top-2) {};

				\node[PUNTO] at (4, -6) (right-1) {};
				\node[PUNTO] at (1.33333, -4) (right-2) {};

				\node[PUNTO] at (-4, -6) (left-1) {};
				\node[PUNTO] at (-1.33333, -4) (left-2) {};

				\path[name path=T21] (right-2) -- (top-2) -- (left-2);
				\path[name path=T22] (right-2) -- (left-2);

				\tikzfillbetween[of=T21 and T22]{white};
				\tikzfillbetween[of=T21 and T22]{pattern={crosshatch dots}, pattern color=gray, radius=0.1pt};

				\draw[fill=black] (top-1) circle[radius=1pt];
				\draw[fill=black] (top-2) circle[radius=1pt]; 

				\node [above=of top-1, yshift=-25pt] {$S$};
				\node [above=of top-2, yshift=-25pt] {$A$};

				\draw[-] (top-1) -- (left-1);
				\draw[-] (right-1) -- (top-1);
				\draw[-] (right-2) -- (left-2);
				\draw[-] (top-2) -- (-2.66666, -6);
				\draw[-] (top-2) -- (2.66666, -6);
				\draw[-] (left-1) -- (-2.66666, -6);
				\draw[-] (right-1) -- (2.66666, -6);
			\end{tikzpicture}
			\caption{Accorciamento}
		\end{subfigure}
	\end{figure}
	% fig 15.5
	Per questo non terminale $A$ vale
	\begin{align*}
		A &\overset{*}{\Rightarrow} w \\
		A &\overset{*}{\Rightarrow} v A x \\
		S &\overset{*}{\Rightarrow} u A y \\
	\end{align*}
	è facile vedere che 
	$$ S \overset{*}{\Rightarrow} u A y \overset{*}{\Rightarrow} u v A x y \overset{*}{\Rightarrow} \dots \overset{*}{\Rightarrow} u v^i A x^i y \overset{*}{\Rightarrow} u v^i w x^i y $$
	quindi abbiamo dimostrato il punto 3.

	La produzione centrale di $A$ deve essere per forza della forma $A \rightarrow B C$, supponiamo che $C$ sia il non terminale sul percorso più lungo che genera $w x$, allora visto che siamo in FNC e non possiamo generare la parola vuota, allora per forza $B$ genera qualcosa diverso da $\varepsilon$, quindi abbiamo dimostrato il punto 2.

	L'altezza della parte dell'albero che genera $vwx$ è al massimo $k + 1$, cioè il numero massimo di nodi che possiamo vedere prima di trovare una ripetizione.
	Quindi utilizzando ancora il lemma di sopra, $|vwx| \leq N$.
\end{proof}
\begin{tcolorbox}[breakable]
	Riprendiamo il linguaggio di prima
	$$ L = \{ a^n b^n c^n \mid c \geq 0 \} $$
	mostriamo che non soddifsa il pumping lemma.

	Supponiamo per assurdo che $L$ sia CF e mostriamo che non può esistere una costante $N$ per cui valga il pumping lemma.
	Sia $N$ la costante di $L$, prendiamo
	$$ z = a^N b^N c^N = u v w x y $$
	Visto che per la prima condizione $|vwx| \leq N$, $vwx$ potrà contenere solo due dei tre simboli, più precisamente $vwx \in a^* b^*$ o $vwx \in b^* c^*$.
	Supponiamo che $vwx \in a^* b^*$, prendiamo $i = 0$ e la stringa $z' = uwy$, questa per la condizione 3 dovrebbe essere in $L$.
	Calcoliamo ora le occorrenze dei simboli in $z'$:
	\begin{align*}
		\#_c(z') &= N \\
		\#_a(z') + \#_b(z') &= 2N - (\#_a(vx) + \#_b(vx)) \\
		                     &\leq 2N \tag*{\tiny Per la condizione 2 $(\#_a(vx) + \#_b(vx)) \geq 1$}
	\end{align*}
	e quindi $z' = a^k b^j c^N \not \in L$ con $k, j < N$, quindi abbiamo un assurdo.
\end{tcolorbox}

\begin{tcolorbox}[breakable]
	Prendiamo il linguaggio
	$$ L = \{ w w \mid w \in \{a, b\}^* \} $$
	questo non è CF.

	Mostriamolo ancora attraverso il pumping lemma.
	Sia $N$ la costante del pumping lemma, scegliamo la stringa
	$$ z = a^N b^N a^N b^N \in L = u v w x y $$
	Utilizznado ancora la condizione 1 abbiamo due casi
	\begin{itemize}
		\item $vwx \in a^* b^* $, prendiamo la stringa $z' = u w x$, questa dovrebbe essere in $L$ per la condizione 3.
			Questa è $z' = a^N b^{N'} a^{N''} b^N$, con $N' \leq N, N'' \leq N$, ora possono essere diminuite solo le $a$, solo le $b$ o entrambe, ma in ogni caso $z' \not \in L$.
		\item $vwx \in b^* a^*$, questo a sua volta dsi divide in due casi, in base al fatto che $vwx$ sia nella prima o nella seconda parte della stringa.
			Prendendo ancora $i = 0$, $z' = a^{N'} b^{N''} a^N b^N$, con $N' \leq N$ e $N'' \leq N$, con ancora almeno uno tra $N'$ e $N''$ minore o uguale a $N$.
	\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[breakable]
	Prendiamo il linguaggio
	$$ L = \{ a^h b^j a^k \mid j = \max(h, k) \} $$
	supponiamo sia CF e chiamiamo $N$ la costante del pumping lemma.
	Prendiamo la stringa
	$$ z = a^N b^N a^N  \in L = u v w x y $$
	Anche qui ci sono due casi
	\begin{itemize}
		\item $vwx \in a^* b^*$, sappiamo che $vx \neq \varepsilon$, distinguiamo tre casi
			\begin{itemize}
				\item $vx \in a^+$, prendendo $i = 2$, otteniamo $z' = a^{N'} b^N a^N$, con $N' > N$, che non fa parte di $L$
				\item $vx \in b^+$, prendendo $i = 0$, otteniamo $z' = a^N b^{N'} a^N$, con $N' < N$, che non fa parte di $L$
				\item $vwx \in a^+b^+$, prendendo $i = 0$, otteniamo $z' = a^{N'} b^{N''} a^N$, con $N'' < N$, che non fa parte di $L$
			\end{itemize}
		\item $vwx \in b^* a^*$, questo caso è simmetrco al precedente
	\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}
	Prendiamo il linguaggio
	$$ L = \{ a^n b^n c^l \mid k \neq n \} $$
	è un linguaggio che rispetta il pumping lemma, ma non è CF.
	% vedremo la prossima lezione.
\end{tcolorbox}

% lezione 16
Bisogna trovare un $i$ tale per cui 
$$ m + (i - 1)(l + r) $$
per cui la somma non sia un numero primo.
Scegliendo $i = m + 1$, allora 
$$ m + m(l + r) = m(l + r + 1) $$
non è primo.

\begin{nota}
Se l'alfabeto è di una lettera sola, non c'è differenza tra regolari e context free.
\end{nota}

\subsection{Lemma di Odgen}

\begin{tcolorbox}[breakable]
Sia
$$ \mathcal{L} = \{ a^n b^n c^k \mid k \neq n \} $$
Intuitivamente non è CF, infatti posso usare una pila per confrontare le $a$ e le $b$, ma una volta fatto questo ho perso l'informazione su $n$.

Mostriamolo con il pumping lemma. 
Scegliamo fissiamo la costante $N$ e una scegliamo una stringa
	$$ z = a^m b^m c^j = u v w x y  \in \mathcal{L} \text{t.c.} |z| \geq N $$
quindi $2m + j \geq N$.

Analizziamo la composizione di $vwx$:
\begin{itemize}
	\item $vwx \in a^+$, questo è facilmente risolvibile mostrando che se si aumentano o diminuiscono le $a$ il loro numero diventa diverso da quello delle $b$
	\item $vwx \in b^+$, è analogo al caso di sopra
	\item $vwx \in c^+$, se $j = 1$ allora facilmente possiamo fissare una $i$ tale che rende il numero delle $c$ uguale a $m$
		Alternativamente possiamo mostrare un caso non valido anche con la stringa 
		$$ z = a^{N + N!} b^{N + N!} c^N $$
		se assumiamo che $|vx| = k$, in
		$$ u v^i w x^i y = a^{N + N!} b^{N + N!} c^{N + k(i - 1)} $$
		e $0 < k \leq N$, vogliamo che $k(i - 1) = N!$, quindi scegliamo $i - 1 = \frac{N!}{k}$, cioè
		$$ i = 1 + \frac{N!}{k} $$
	\item $vwx \in a^+b^+$, questo a sua volta si divide in vari sottocasi
		\begin{itemize}
			\item $v \in a^+ b^+$, cioè il confine tra $a$ e $b$ cade in $v$, è facile mostrare che $v^i$ sarebbe una stringa composta da $a$ seguite da $b$ seguite ancora da $a$ e così via
			\item $w \in a^+ b^+$, allora $v \in a^*$ e $x \in b^*$ abbiamo ancora altri casi
				\begin{itemize}
					\item se $v$ e $x$ sono di dimensione diversa è facile
					\item se $v$ e $x$ sono di dimensione uguale, cioè
						$$ v = a^h, b = b^h $$
				\end{itemize}
		\end{itemize}
\end{itemize}
In questo esempio il pumping lemma non si può applicare. % ???

\end{tcolorbox}

Sia $T$ un albero con alcune foglie marcate.
E definiamo dei nodi interni speciali detti \textit{branch point} definiti come nodi che hanno almeno due figli marcati o due figli con discendenti marcati.
Definiamo i \textit{nodi speciali} come tutti i branch point e i nodi che hanno almeno un figlio marcato.
% 16.1
\begin{center}
	% rendo più distinguibili i nodi di diverso tipo (e.g. bianco e nero)
	\begin{tikzpicture}[
			MARKED/.style={fill=red, circle, inner sep=2pt},
			BP/.style={fill=yellow, circle, inner sep=2pt},
			SPECIAL/.style={fill=orange, circle, inner sep=2pt},
			NORMAL/.style={draw, fill=white, circle, inner sep=2pt},
			scale=.6, every node/.style={transform shape}
		]
		\node[NORMAL] (root) {}
			[sibling distance=6cm]
			child { 
				[sibling distance=4.5cm]
				node[BP] {}
				child {
					[sibling distance=1.5cm]
					node[NORMAL] {}
					child {
						node[BP] {}
						child {
							node[NORMAL] {}
							child { 
								node[SPECIAL] {} 
								child { node[MARKED] {} } 
							}
							child { node[NORMAL] {} }
						}
						child { node[MARKED] {} }
					}
					child { node[NORMAL] {} }
				}
				child {
					 [sibling distance=2cm]
					 node[BP] {}
					 child {
						[sibling distance=1cm]
						node[NORMAL] {}
						child { node[NORMAL] {} }
						child { 
							node[NORMAL] {}
							child {
								node[NORMAL] {}
					 			child {
									node[SPECIAL] {}
					 				child { node[MARKED] {} }
					 			}
							}
						}
						child { node[NORMAL] {} }
					 }
					 child {
					 	[sibling distance=1cm]
						node[NORMAL] {}
					 	child[missing]
					 	child {
							node[NORMAL] {}
							child { node[NORMAL] {} }
					 		child {
					 			node[SPECIAL] {}			
					 			child { node[MARKED] {} }
								child { node[NORMAL] {} }
					 		}
						}
					}
				}
			}
			child { 
				[sibling distance=2cm]
				node[NORMAL] {}
			 	child { node[NORMAL] {} 
					child { node[NORMAL] {} }
				}
				child { 
					node[NORMAL] {}
					child {
						node[NORMAL] {}
						child { node[NORMAL] {} }
					}
				}
			};
		\node at (4, -8) {
			\begin{tblr}{|cl|}
				\hline
				\textcolor{red}{$\blacksquare$} & Nodi marcati \\
				\textcolor{orange}{$\blacksquare$} & Nodi speciali \\
				\textcolor{yellow}{$\blacksquare$} & Branch point \\
				\hline
			\end{tblr}
		};
	\end{tikzpicture}
\end{center}
supponiamo di marcare un altro nodo
% 16.2
\begin{center}
	% rendo più distinguibili i nodi di diverso tipo (e.g. bianco e nero)
	\begin{tikzpicture}[
			MARKED/.style={fill=red, circle, inner sep=2pt},
			BP/.style={fill=yellow, circle, inner sep=2pt},
			SPECIAL/.style={fill=orange, circle, inner sep=2pt},
			NORMAL/.style={draw, fill=white, circle, inner sep=2pt},
			scale=.6, every node/.style={transform shape}
		]
		\node[BP] (root) {}
			[sibling distance=6cm]
			child { 
				[sibling distance=4.5cm]
				node[BP] {}
				child {
					[sibling distance=1.5cm]
					node[NORMAL] {}
					child {
						node[BP] {}
						child {
							node[NORMAL] {}
							child { 
								node[SPECIAL] {} 
								child { node[MARKED] {} } 
							}
							child { node[NORMAL] {} }
						}
						child { node[MARKED] {} }
					}
					child { node[NORMAL] {} }
				}
				child {
					 [sibling distance=2cm]
					 node[BP] {}
					 child {
						[sibling distance=1cm]
						node[NORMAL] {}
						child { node[NORMAL] {} }
						child { 
							node[NORMAL] {}
							child {
								node[NORMAL] {}
					 			child {
									node[SPECIAL] {}
					 				child { node[MARKED] {} }
					 			}
							}
						}
						child { node[NORMAL] {} }
					 }
					 child {
					 	[sibling distance=1cm]
						node[NORMAL] {}
					 	child[missing]
					 	child {
							node[NORMAL] {}
							child { node[NORMAL] {} }
					 		child {
					 			node[SPECIAL] {}			
					 			child { node[MARKED] {} }
								child { node[NORMAL] {} }
					 		}
						}
					}
				}
			}
			child { 
				[sibling distance=2cm]
				node[NORMAL] {}
			 	child { node[NORMAL] {} 
					child { node[NORMAL] {} }
				}
				child { 
					node[NORMAL] {}
					child {
						node[SPECIAL] {}
						child { node[MARKED] {} }
					}
				}
			};
	\end{tikzpicture}
\end{center}
\begin{lemma}
  	Sia $G = \langle V, \Sigma, P, S\rangle$ una grammatica in FNC e prendiamo un albero di derivazione 
  	$$A \overset{*}{\Rightarrow} w, \hspace{1cm} A \in V, w \in \Sigma^* $$
  	e $w$ contiene alcune posizioni marcate.
  
  	Se il numero massimo di nodi speciali su un cammino dalla radice alle foglie è minore o uguale a $k$, allora il numero di posizioni marcate in $w$ è $\leq 2^k - 1$.
\end{lemma}
Questo è una generalizzazione del lemma della lezione precedente, nell'altro supponevamo marcata l'intera stringa.
% 
\begin{proof}
 	Procediamo per induzione su $k$
 	\begin{itemize}
 		\item $k = 1$, quindi in ogni cammino dalla radice ad una foglia esiste al massimo un nodo speciale.
 			% 16.3
 			Supponiamo che questo nodo speciale sia un branch point
 			% 16.4
 			ma questo non può valere visto che in FNC l'unica produzione che può generare un terminale è della forma $C \Rightarrow a$, quindi anche questi due dovrebbero essere nodi speciali.
 			Quindi esiste una singola foglia marcata, infatti se ne esistesse più di una, allora il loro nodo in comune sarebbe un branch point.
 		\item supponiamo che sia vero per valori $< k$, e mostriamo che è vero per $k$.
 			% 16.5
 			Fermiamoci al primo nodo speciale dalla radice, e che questo sia un branch point, per ipotesi induttiva nell'abero di sinistra e di destra ci saranno al massimo $2^{k - 2}$ posizioni marcate.
 			Inoltre $z_0$ e $z_3$ non possono avere posizioni marcate, altrimenti il branch point sarebbe più in alto.
 			Quindi $\leq 2^{k - 2} + 2^{k - 2} = 2^{k - 1}$.
	\end{itemize}
\end{proof}

\begin{lemma}[Lemma di Ogden]
	\label{lemma:ogden}
	Sia $L \in \text{CF}$, allora esiste una costante $N$ tale che per ogni $z \in L$ sono marcate $N$ posizioni e possiamo scomporre $z$ tale che
	$$ z = u v w x y $$
	tale che
	\begin{itemize}
		\item $vx$ contiene almeno una posizione marcata
		\item $vwx$ contiene al più $N$ posizioni marcate
		\item per ogni $i > 0$, $u v^i w x^i y \in L$
	\end{itemize}
\end{lemma}
Quindi il pumping lemma è un caso speciale di questo in cui ogni posizione è marcata.
\begin{proof}
	Sia $G = \langle V, \Sigma, S, P \rangle$ una grammatica in FNC, definiamo $k = |V|$ e fissiamo $N = 2^k$.
	Prendiamo una stringa $z \in L$ con almeno $N$ nodi marcati.
	Prendiamo il cammino dalle foglie alla radice che contiene il maggior numero di nodi speciali.
	In base al lemma di prima, il massimo numero di nodi speciali sul cammino è $\geq k + 1$.
	Percorrendo il cammino e leggendo le variabili che compaiono sui nodi speciali, sia questa $A$, troveremo almeno una ripetizione.
	I sottoalberi di questa coppia di definisce le nostre parti $u, v, w, x$ e $y$.

	Visto che la prima occorrenza di $A$ sarà un branch point, questa sarà una produzione del tipo $A \rightarrow BC$, uno dei due rami potrerà alla seconda $A$ e sicuramente il secondo porterà ad una foglia marcata, quindi in $v$ e $x$ c'è almeno un branch point.
\end{proof}

Con questo nuovo lemma proviamo a dimostrare l'esempio di prima
\begin{tcolorbox}[breakable]
	Sia
	$$ L = \{ a^n b^n c^k \mid k \neq n \} $$
	con $N$ costante per $L$.
	La stringa con cui si può arrivare ad un assurdo è
	$$ z = a^N b^N c^{N + N!} = uvwxy $$
	con tutte le $a$ marchiate.

	Visto che $vx$ deve contenere almeno una posizione marcata, allora questo deve avere almeno una $a$ al suo interno.
	Questo ci restringe ai tre casi
	\begin{itemize}
		\item $vwx \in a^+$, il numero di $a$ cresce, ma non il numero di $b$
		\item $vwx \in a^+ b^+$, si divide in alcuni sottocasi in base a dove il confine tra le $a$ e $b$ cade
			\begin{itemize}
				\item $v \in a^+ b^+$, ripetere $v$ fa sì che la stringa perda la struttura e porta ad avere $a$ dopo le $b$
				\item analogo a sopra se il confine si trova su $x$
				\item $v \in a^+$ e $x \in b^+$, supponiamo più precisamente che
					$$ v = a^l, x = b^r $$
					\begin{itemize}
						\item se $l \neq r$ è ovvio che la stringa non sia valida, anche solo per $i = 0$ cancellerei un numero diverso di $a$ e di $b$
						\item se $l = r$, la stringa che otterremmo ripetendo $i$ volte sarebbe
							$$ a^{N + l(i - 1)} b^{N + l(r - 1)} c^{N + N!} $$
							se si scegle $i = 1 + \frac{N!}{l}$ allora otteniamo che le $a$ e le $b$ sono uguali al numero di $c$ e $N!$ è sicuramente divisibile da $l$.
					\end{itemize}
			\end{itemize}
		\item $vwx \in a^+ b^N c^+ $, 
			\begin{itemize}
				\item se $v$ e $x$ contengono tipi di lettere diverse, come prima ripetendo si perde la struttura
				\item per il resto si tratta in maniera analogo a il secondo caso
			\end{itemize}
	\end{itemize}
\end{tcolorbox}

% esercizi
\begin{tcolorbox}[breakable]
	Sia
	$$ \mathcal{L} = \{ a^p b^q c^r \mid p = q \vee q = r \} $$
	e 
	$$ \mathcal{L} = \{ a^p b^q c^r \mid p = q \otimes q = r \} $$
\end{tcolorbox}

% lezione 17
\section{Ambiguità}
Prendiamo il linguaggio
$$ \mathcal{L} = \{ a^p b^q c^r \mid p = q \vee q = r \} $$
sappiamo gli automi per $a^n b^n c^m$ e per $a^m b^n c^n$, % ref ?
quindi nondeterministicamente all'inizio scegliamo quale strada prendere nell'automa.
Lo stesso si può fare con una grammatica
$$ S \rightarrow S' C \mid A S'' $$
dove $S' \rightarrow a S' b \mid \varepsilon $ genera $a^n b^n$
e $S'' \rightarrow b S'' c \mid \varepsilon $ genera $b^n c^n$
e 
\begin{align*}
	C &\rightarrow c C \mid \varepsilon \\
	A &\rightarrow a A \mid \varepsilon \\
\end{align*}
\begin{center}
	\begin{tikzpicture}
		\node {$S$}
			[sibling distance=3cm, level distance=1cm]
			child {
				node {$S'$}
				[sibling distance=1cm]
				child { node {$a$} }
				child {
					node {$S'$}
					child { node {$\varepsilon$} }
				}
				child { node {$b$} }
			}
			child {
				node {$C$}
				[sibling distance=1cm]
				child { node {$c$} }
				child {
					node {$C$}
					child { node {$c$} }
					child { 
						node {$C$} 
						child { node {$\varepsilon$} }
					}
				}
			};
	\end{tikzpicture}
\end{center}
% fig 17.1
vediamo ora i casi particolari dei due alberi
\begin{figure}[H]
	\centering
	\begin{subfigure}{.45\textwidth}
		\begin{tikzpicture}
			\node {$S$}
				[sibling distance=3cm, level distance=1cm]
				child {
					node {$S'$}
					[sibling distance=1cm]
					child { node {$a$} }
					child {
						node {$S'$}
						child { node {$a$} }
						child {
							node {$S'$}
								child { node {$\varepsilon$} }
						}
						child { node {$b$} }
					}
					child { node {$b$} }
				}
				child {
					node {$C$}
					[sibling distance=1cm]
					child { node {$c$} }
					child {
						node {$C$}
						child { node {$c$} }
						child { 
							node {$C$} 
							child { node {$\varepsilon$} }
						}
					}
				};
		\end{tikzpicture}
	\end{subfigure}
	\begin{subfigure}{.45\textwidth}
		\begin{tikzpicture}
			\node {$S$}
				[sibling distance=3cm, level distance=1cm]
				child {
					node {$A$}
					[sibling distance=1cm]
					child { node {$a$} }
					child {
						node {$A$}
						child { node {$a$} }
						child {
							node {$A$}
							child { node {$\varepsilon$} }
						}
					}
				}
				child {
					node {$S''$}
					[sibling distance=1cm]
					child { node {$b$} }
					child {
						node {$S''$}
						child { node {$b$} }
						child { 
							node {$S''$}
							child { node {$\varepsilon$} }
						}
						child { node {$c$} }
					}
					child { node {$c$} }
				};
		\end{tikzpicture}
	\end{subfigure}
	\caption{Alberi per la stringa $a^2 b^2 c^2$}
\end{figure}
Entrambi gli alberi rappresentano derivazioni leftmost che generano la stessa stringa $a^2 b^2 c^2$. % mostro magari le due derivazioni leftmost

Una grammatica viene detta \textit{ambigua} se esiste almeno una stringa che può essere generata in due modi diversi.
\begin{definizione}[Grado di ambiguità]
	Chiamiamo il \textit{grado di ambiguità} di una stringa $w \in \Sigma^*$ in $G$ è uguale al numero di alberi di derivazione di $w$ in $G$.
	Ed il grado di ambiguità di $G$ è il massimo grado di ambiguità generato dalle stringhe della grammatica.
\end{definizione}

Non sempre si può trovare una grammatica non ambigua per un linguaggio.
Esistono linguaggi detti \textit{inerentemente ambigui}, cioè per cui ogni grammatica di tipo 2 che li genera sarà ambigua.

Utilizzando il lemma di Ogden mostreremo che il linguaggio $L$ è interentemente ambiguo.
Mostriamo prima una versione alternativa del lemma
\begin{lemma}[Lemma di Ogden]
	Sia $G$ una grammatica CF e sia $L = L(G)$.
	Esiste una costante $N$ tale che per ogni $z \in L$ con almeno $N$ posizioni marcate possiamo decomporre $z$ in 5 parti
	$$ z = uvwxy $$
	tali che
	\begin{enumerate}
		\item $vwx$ contiene al più $N$ posizioni marcate
		\item $vx$ contiene almeno una posizione marcata
		\item esiste una variabile $A \in V$ tale che $S \overset{*}{\Rightarrow} uAy$, e $A \overset{*}{\Rightarrow} vAx$ e $A \overset{*}{\Rightarrow} w$.
			E dunque per ogni $i \geq 0$ $u v^i w x^i y \in L$.
	\end{enumerate}
\end{lemma}
Noi il teorema di Ogden lo abbiamo dimostrato utilizzando grammatiche in CNF, si può fare anche per grammatiche generiche, ma è più tedioso.

\begin{proof}[Ambiguità di $L$]
	Noi vogliamo dimostrare che
	$$ \mathcal{L} = \{ a^p b^q c^r \mid p = q \vee q = r \} $$
	è inerentemente ambiguo.
	
	Sia $G$ una qualunque grammatica CF per $\mathcal{L}$.
	Sia $N$ la costante del lemma di Ogden e $m = \max(N, 3) $ il massimo tra $N$ e 3.
	Prendiamo 
	$$ z = a^m b^m c^{m + m!} \in \mathcal{L}$$
	e marchiamo tutte le $a$.

	Prendiamo la stringa$\alpha$ ottenuta ponendo $i = 2$, cioè la stringa $\alpha = u v^2 w x^2 y \in \mathcal{L}$.
	Contiamo le $b$ in $\alpha$
	\begin{align*}
		\#_b(\alpha) &= \underbrace{\#_b(z)}_{m} + \underbrace{\#_b(vx)}_{\leq m} \\
		&\leq 2m \\ 
		&< m + m! \tag{Visto che $m$ è almeno 3} \\
		&\leq \#_c(\alpha) \tag{Le $c$ rimaste sono almeno quelle che c'erano prima}
	\end{align*}
	e visto che $uv^2 w x^2 y \in \mathcal{L}$ allora $\#_b(\alpha) = \#_a(\alpha)$ e per la proprietà 2 del lemma di Ogden $\#_a(vx) = \#_b(vx) \geq 1$.
	Inoltre affinché $u v^2 w x^2 y \in \mathcal{L}$ sia ancora in $\mathcal{L}$ deve valere che
	\begin{align*}
		v &= a^l \\
		x &= b^l \\
	\end{align*}
	con $1 \leq l \leq m$.
	Altrimenti ripetendo perderemmo la struttura.

	Prendiamo ora
	$$ u v^i w x^i z = a^{m + (i - 1)l}b^{m + (i - 1)l} c^{m + m!} \in \mathcal{L} $$
	scegliendo $i = 1 + \frac{m!}{l}$ otteniamo la stringa 
	$$ \beta = a^{m + m!} b^{m + m!} c^{m + m!} $$
	che appartiene al nostro linguaggio.
	\begin{figure}[H]
		\centering
		\begin{subfigure}{.3\textwidth}
			\centering
			\begin{tikzpicture}[
					PUNTO/.style={minimum size=0, inner sep=0, outer sep=0}, 
					scale=0.45, 
					every node/.style={transform shape}
				]
				\node[PUNTO] at (0, 0) (top-1) {};
				\node[PUNTO] at (0, -2) (top-2) {};

				\node[PUNTO] at (4, -4) (right-1) {};
				\node[PUNTO] at (2, -4) (right-2) {};

				\node[PUNTO] at (-4, -4) (left-1) {};
				\node[PUNTO] at (-2, -4) (left-2) {};

				\node [above=of top-1, yshift=-25pt] {$S$};
				\node [above=of top-2, yshift=-25pt] {$A$};

				\draw[-] (top-1) -- (left-1);
				\draw[-] (right-1) -- (top-1);
				\draw[-] (top-2) -- (left-2);
				\draw[-] (right-2) -- (top-2);
				\draw[-] (left-1) -- (left-2);
				\draw[-] (right-1) -- (right-2);

				\draw[decorate, decoration={brace, mirror, amplitude=0.2cm}] 
					([yshift=-5pt] left-1.south) to node[below, yshift=-.5cm] {$u = a^+$} ([yshift=-5pt] left-2.south);
				\draw[decorate, decoration={brace, mirror, amplitude=0.2cm}] 
					([yshift=-5pt] right-2.south) to node[below, yshift=-.5cm] {$y = c^+$} ([yshift=-5pt] right-1.south);
			\end{tikzpicture}
		\end{subfigure}
		\begin{subfigure}{.3\textwidth}
			\centering
			\begin{tikzpicture}[
					PUNTO/.style={minimum size=0, inner sep=0, outer sep=0}, 
					scale=0.45, 
					every node/.style={transform shape}
				]
				\node[PUNTO] at (0, 0) (top-1) {};
				\node[PUNTO] at (0, -2) (top-2) {};

				\node[PUNTO] at (4, -4) (right-1) {};
				\node[PUNTO] at (2, -4) (right-2) {};

				\node[PUNTO] at (-4, -4) (left-1) {};
				\node[PUNTO] at (-2, -4) (left-2) {};

				\node [above=of top-1, yshift=-25pt] {$A$};
				\node [above=of top-2, yshift=-25pt] {$A$};

				\draw[-] (top-1) -- (left-1);
				\draw[-] (right-1) -- (top-1);
				\draw[-] (top-2) -- (left-2);
				\draw[-] (right-2) -- (top-2);
				\draw[-] (left-1) -- (left-2);
				\draw[-] (right-1) -- (right-2);

				\draw[decorate, decoration={brace, mirror, amplitude=0.2cm}] 
					([yshift=-5pt] left-1.south) to node[below, yshift=-.5cm] {$v = a^i$} ([yshift=-5pt] left-2.south);
				\draw[decorate, decoration={brace, mirror, amplitude=0.2cm}] 
					([yshift=-5pt] right-2.south) to node[below, yshift=-.5cm] {$x = b^i$} ([yshift=-5pt] right-1.south);
			\end{tikzpicture}
		\end{subfigure}
		\begin{subfigure}{.3\textwidth}
			\centering
			\begin{tikzpicture}[
					PUNTO/.style={minimum size=0, inner sep=0, outer sep=0}, 
					scale=0.45, 
					every node/.style={transform shape}
				]
				\node[PUNTO] at (0, 0) (top-1) {};
				\node[PUNTO] at (0, -2) (top-2) {};

				\node[PUNTO] at (2, -3) (right-1) {};

				\node[PUNTO] at (-2, -3) (left-1) {};

				\node [above=of top-1, yshift=-25pt] {$A$};

				\draw[-] (top-1) -- (left-1);
				\draw[-] (right-1) -- (top-1);
				\draw[-] (left-1) -- (right-1);

				\draw[decorate, decoration={brace, mirror, amplitude=0.2cm}] 
					([yshift=-5pt] left-1.south) to node[below, yshift=-.5cm] {$w = a^+ b^+$} ([yshift=-5pt] right-1.south);
			\end{tikzpicture}
		\end{subfigure}
	\end{figure}
	% fig 17.4
	Nella stringa $\beta$ la maggior parte delle $b$ è ottenuta replicando il secondo sottoalbero.

	Partiamo invece da una stringa
	$$ z' = a^{m + m!} b^m c^m $$
	e marco tutte le $c$.
	Scomponiamola in
	$$ u' v' w' x' y' $$
	e utilizzando i ragionamenti di prima mostriamo che $v' = b^j$ e $x' = c^j$ e ripetendo $i$ volte $v'$ e $x'$ otteniamo
	$$ a^{m + m!} b^{m + (i - 1)j} c^{m + (i - 1)j} $$
	e scegliendo $i = 1 + \frac{m!}{j}$ come prima otteniamo $\beta$.
	Come prima possiamo immaginare alberi di forma
	% fig 17.5
	\begin{figure}[H]
		\centering
		\begin{subfigure}{.3\textwidth}
			\centering
			\begin{tikzpicture}[
					PUNTO/.style={minimum size=0, inner sep=0, outer sep=0}, 
					scale=0.45, 
					every node/.style={transform shape}
				]
				\node[PUNTO] at (0, 0) (top-1) {};
				\node[PUNTO] at (0, -2) (top-2) {};

				\node[PUNTO] at (4, -4) (right-1) {};
				\node[PUNTO] at (2, -4) (right-2) {};

				\node[PUNTO] at (-4, -4) (left-1) {};
				\node[PUNTO] at (-2, -4) (left-2) {};

				\node [above=of top-1, yshift=-25pt] {$S$};
				\node [above=of top-2, yshift=-25pt] {$A'$};

				\draw[-] (top-1) -- (left-1);
				\draw[-] (right-1) -- (top-1);
				\draw[-] (top-2) -- (left-2);
				\draw[-] (right-2) -- (top-2);
				\draw[-] (left-1) -- (left-2);
				\draw[-] (right-1) -- (right-2);

				\draw[decorate, decoration={brace, mirror, amplitude=0.2cm}] 
					([yshift=-5pt] left-1.south) to node[below, yshift=-.5cm] {$u' = a^+$} ([yshift=-5pt] left-2.south);
				\draw[decorate, decoration={brace, mirror, amplitude=0.2cm}] 
					([yshift=-5pt] right-2.south) to node[below, yshift=-.5cm] {$y' = c^+$} ([yshift=-5pt] right-1.south);
			\end{tikzpicture}
		\end{subfigure}
		\begin{subfigure}{.3\textwidth}
			\centering
			\begin{tikzpicture}[
					PUNTO/.style={minimum size=0, inner sep=0, outer sep=0}, 
					scale=0.45, 
					every node/.style={transform shape}
				]
				\node[PUNTO] at (0, 0) (top-1) {};
				\node[PUNTO] at (0, -2) (top-2) {};

				\node[PUNTO] at (4, -4) (right-1) {};
				\node[PUNTO] at (2, -4) (right-2) {};
3
				\node[PUNTO] at (-4, -4) (left-1) {};
				\node[PUNTO] at (-2, -4) (left-2) {};

				\node [above=of top-1, yshift=-25pt] {$A'$};
				\node [above=of top-2, yshift=-25pt] {$A'$};

				\draw[-] (top-1) -- (left-1);
				\draw[-] (right-1) -- (top-1);
				\draw[-] (top-2) -- (left-2);
				\draw[-] (right-2) -- (top-2);
				\draw[-] (left-1) -- (left-2);
				\draw[-] (right-1) -- (right-2);

				\draw[decorate, decoration={brace, mirror, amplitude=0.2cm}] 
					([yshift=-5pt] left-1.south) to node[below, yshift=-.5cm] {$v' = b^j$} ([yshift=-5pt] left-2.south);
				\draw[decorate, decoration={brace, mirror, amplitude=0.2cm}] 
					([yshift=-5pt] right-2.south) to node[below, yshift=-.5cm] {$x' = c^j$} ([yshift=-5pt] right-1.south);
			\end{tikzpicture}
		\end{subfigure}
		\begin{subfigure}{.3\textwidth}
			\centering
			\begin{tikzpicture}[
					PUNTO/.style={minimum size=0, inner sep=0, outer sep=0}, 
					scale=0.45, 
					every node/.style={transform shape}
				]
				\node[PUNTO] at (0, 0) (top-1) {};

				\node[PUNTO] at (2, -3) (right-1) {};

				\node[PUNTO] at (-2, -3) (left-1) {};

				\node [above=of top-1, yshift=-25pt] {$A'$};

				\draw[-] (top-1) -- (left-1);
				\draw[-] (right-1) -- (top-1);
				\draw[-] (left-1) -- (right-1);

				\draw[decorate, decoration={brace, mirror, amplitude=0.2cm}] 
					([yshift=-5pt] left-1.south) to node[below, yshift=-.5cm] {$w' = b^+ c^+$} ([yshift=-5pt] right-1.south);
			\end{tikzpicture}
		\end{subfigure}
	\end{figure}
	Qui ancora la maggior parte delle $b$ è generata dal secondo albero.
	Ma visto che l'albero di $A$ genera anche $a$ e l'albero di $A'$ genera anche $c$ i due sono per forza diversi.
\end{proof}

Possiamo dare una definizione analoga di ambiguità per gli automi a pila.
\begin{definizione}
	Un PDA è ambiguo se esiste una stringa con due computazioni accettanti differenti.
\end{definizione}
\`E facile vedere attraverso la trasformazione da grammatica ad automa che il numero di computazioni diverse è uguale al numero di alberi leftmost diversi, visto che si simulava la derivazione leftmost.
Dal lato opposto è più difficile da mostrare e soprattutto la trasformazione non preserva il grado di ambiguità.
\begin{figure}[H]
	\centering
	\begin{tikzpicture}
		\draw[->] (-1, 0) -- (-1, 4) node[left, yshift=-0.2cm] {\rotatebox{90}{\tiny pila}};
		\draw[->] (-1, 0) -- (7, 0) node[below, xshift=-0.2cm] {\tiny input};

		\node at (0, 0) [circle,fill,inner sep=1.5pt] {};
		\node at (1.5, 0) [circle,fill,inner sep=1.5pt] {};
		\node at (3, 0) [circle,fill,inner sep=1.5pt] {};
		\node at (4.5, 0) [circle,fill,inner sep=1.5pt] {};
		\node at (6, 0) [circle,fill,inner sep=1.5pt] {};

		\tikzmath {
			real \x, \rand, \precx, \precy;
			\precx = 0;
			\precy = 0;
			for \x in {0.2,0.4,...,1.3} {
				\rand = 1 + (random(0, 100) / 66);
				{ \draw[-] (\precx, \precy) -- (\x, \rand); };
				\precx = \x;
				\precy = \rand;
			};
			{ \draw[-] (\precx, \precy) -- (1.5, 0); };
			\precx = 1.5;
			\precy = 0;
			for \x in {1.7,1.9,...,2.8} {
				\rand = 1 + (random(0, 100) / 66);
				{ \draw[-] (\precx, \precy) -- (\x, \rand); };
				\precx = \x;
				\precy = \rand;
			};
			{ \draw[-] (\precx, \precy) -- (3, 0); };
			\precx = 3;
			\precy = 0;
			for \x in {3.2,3.4,...,4.3} {
				\rand = 1 + (random(0, 100) / 66);
				{ \draw[-] (\precx, \precy) -- (\x, \rand); };
				\precx = \x;
				\precy = \rand;
			};
			{ \draw[-] (\precx, \precy) -- (4.5, 0); };
			\precx = 4.5;
			\precy = 0;
			for \x in {4.7,4.9,...,5.8} {
				\rand = 1 + (random(0, 100) / 66);
				{ \draw[-] (\precx, \precy) -- (\x, \rand); };
				\precx = \x;
				\precy = \rand;
			};
			{ \draw[-] (\precx, \precy) -- (6, 0); };
		}
	\end{tikzpicture}
\end{figure}
Si può vedere dal grafico infatti che la pila può essere scomposta in vari modi equivalenti ($1(2(34))$, $(12)(34)$, $(((1)2)3)4$, e così via). % chiarisco meglio
Quindi parlare di ambiguità negli automi a pila e nelle grammatiche di tipo 2 è equivalente.
E di conseguenza un linguaggio inerentemente ambiguo avrà sia grammatica che automa a pila ambigui.

Perché un automa a pila ammetta ambiguità questo deve per forza essere nondeterministico.
Quindi
$$ \mathcal{L} = \{ a^p b^q c^r \mid p = q \vee q = r \} $$
necessita il nondeterminismo.

Richiamiamo la definizione di automa a pila deterministico:
$$ M = \langle Q, \Sigma, \Gamma, \delta, q_0, Z_0, F \rangle $$
è deterministico se ad ogni passo possiamo fare una singola scelta, cioè sse
\begin{itemize}
	\item $\forall q \in Q, A \in \Gamma \mid \delta(q, \varepsilon, A) \neq \varnothing \Rightarrow \forall a \in \Sigma \delta(q, a, A) = \varnothing $
	\item $\forall q \in Q, A \in \Gamma, a \in \Sigma \cup \{ \varepsilon \} \mid | \delta(q, a, A) | = 1 $
\end{itemize}
mostriamo ora che i due criteri di accettazione sono diversi per automi deterministici, specificamente la accettazione per pila vuota è più debole.
Supponiamo infatti di avere un automa a pila che accetta per pila vuota il linguaggio regolare $(aa)^+$.
Siccome l'automa deve accettare $aa$ dopo questa avrà la pila vuota.
Ma da una pila vuota non può più continuare, quindi non può accettare $aaaa$ ad esempio.
Quindi i linguaggi che accetta sono tutti quelli le cui stringhe non hanno come prefissi altre stringhe del linguaggio, perché nel riconoscere il prefisso svuoterà la pila e non potrà più andare avanti.
Questi in teoria dei codici sono detti codici prefissi e contengono alcuni regolari e alcuni context free.

Di solito si ovvia a questo utilizzando un simbolo finale non nel linguaggio, ad esempio $(aa)^+ \#$, in modo tale che la pila non si svuoti completamente prima di arrivare al marcatore finale.

Quindi da ora in poi quando parliamo di DCFL, cioè i linguaggi CF deterministici, intendiamo i linguaggi riconosciuti per stati finali.
Questi contengono per forza i linguaggi regolari, perché semplicemente possiamo non utilizzare la pila.
Vale che
$$ \text{Reg} \subset \text{DCFL} \subset \text{CFL} $$
perché $\{ a^p b^q c^r \mid p = q \vee q = r \} \in \text{CFL}$ ma $\not \in \text{DCFL}$.

Abbiamo visto che l'ambiguità necessità del nondeterminismo.
Possiamo chiederci l'inverso, cioè se un linguaggio necessita del nondeterminismo allora questo è per forza ambiguo.
Un linguaggio che necessita del nondeterminismo è quello dei palindromi (non ancora dimostrato), o -- per semplicità -- dei palindromi pari
$$ L = \{ w w^R \mid w \in \{a, b\}^* \} $$
L'automa deve ``scommettere'' su quando è arrivato a metà , quindi il nondeterminismo è necessario, ma la metà è una sola, qiundi non è ambiguo.
\`E anche facile vederlo dalla grammatica che è
	$$ S \rightarrow a S a \mid b S b \mid \varepsilon $$
Quindi il nondeterminismo non implica la ambiguità.

Non studieremo il determinismo per le grammatiche perché questo è più strano e ammette miriadi di classi diverse.

\section{Operazioni e chiusura con i linguaggi CF}\label{sec:operazioni di chiusura con i linguaggi CF}
\begin{table}[H]
	\centering
	\begin{longtblr}[
			note{1} = {Risultati non mostrati a lezione},
			caption = Chiusura delle opreazioni,
			entry = none
		]{ colspec = {|c|c|c|}}
		\hline
		Operazione & CFL? & DCFL? \\
		\hline
		\hline
		Unione & Sì & No \\
		Intersezione & No & No \\
		Intersezione con un regolare & Sì & Sì \\
		Complemento & No & Sì \\
		Prodotto & Sì & No \\
		Chiusura di Kleene & Sì & No \\
		Morfismo & Sì & No \\
		Sostituzione & Sì & No \\
		Reversal\TblrNote{1} & Sì & No \\	% idk l'ha detto e basta
		Shuffle\TblrNote{1} & No & No  \\	% idk l'ha detto e basta
		\hline
	\end{longtblr}
\end{table}

\paragraph{Unione} Date due grammatiche $G' = \langle V', \Sigma, P', S' \rangle$ e $G'' = \langle V'', \Sigma, P'', S'' \rangle$ con $V' \cap V'' = \varnothing$, definiamo la unione delle due come 
$$ G = \langle V' \cup V'' \cup \{ S \}, \Sigma, P' \cup P'' \cup \{ S \rightarrow S', S \rightarrow S'' \}, S \rangle $$
e nel caso degli automi possiamo fare la scelta all'inizio.
La unione è chiusa per i CFL e non chiusa per i CDFL, infatti dati $L' = \{ a^n b^n c^m \} \in \text{DCFL}$ e $L'' = \{ a^m b^n c^n \} \in \text{DCFL}$ la loro unione $L' \cup L'' = \{ a^p b^q c^r \mid p = q \vee q = r \}$ è ambigua, quindi nondeterministica.

\paragraph{Intersezione} La intersezione non è chiusa, infatti $L' \cap L'' = \{ a^n b^n c^n \}$ che abbiamo dimostrato che non è CF.
Inoltre non possiamo utilizzare il metodo dei FSA di far andare in parallelo i due automi, qui i due due automi interferirebbero nel loro utilizzo della pila.

\paragraph{Intersezione con un regolare} Possiamo usare il metodo degli FSA di far andare in parallelo i due automi, quindi incorporo nel controllo del PDA l'FSA.

\paragraph{Complemento} Se il complemento fosse chiuso, potremmo ottenere una intersezione chiusa attraverso l'unione e De Morgan, quindi il complemento deve per forza non essere chiuso.
Il complemento per i deterministici invece è chiuso.
% lezione 18
\begin{tcolorbox}[breakable]
	Esibiamo ora un controesempio che mostra che il complemento non è chiuso rispetto ai CF.
	Prendiamo il linguaggio
	$$ L = \{ x \in \{a, b\}^* \mid \forall w : x \neq w w \} \in \text{CF} $$
	il complemento di questo è
	$$ L^C = \{ w w \mid w \in \{a, b\}^* \} $$
	che come abbiamo già mostrato in passato non è context free.	% ci dovrebbe essere un esercizio in cui mostriamo che il linguaggio L è CF

	Per completezza mostriamo anche che $L$ sia effettivamente CF costruendo un automa a pila che lo riconosce.
	Questo -- molto brevemente -- scommette sulla posizione dei due caratteri diversi.
	\begin{center}
		\begin{tikzpicture}[
				PUNTO/.style={minimum size=0, inner sep=0, outer sep=0}, 
			]
			\node[PUNTO] at (0, 0) (left) {};
			\node[PUNTO] at (4, 0) (middle) {};
			\node[PUNTO] at (8, 0) (right) {};

			\node[PUNTO, label=below:{\tiny $i$}] at (1, 0) (i) {};
			\node[PUNTO, label=below:{\tiny $i + n$}] at (5, 0) (in) {};

			\draw (i) circle[radius=2pt];
			\draw (in) circle[radius=2pt];

			\draw[|-|] (left) -- node {\tiny $|$} (right);

			\draw[decorate, decoration={brace, mirror, amplitude=.2cm}] ([yshift=-.35cm]left.south) to node[below, yshift=-.2cm] {\tiny $n'$} ([yshift=-.35cm]i.south);
			\draw[decorate, decoration={brace, mirror, amplitude=.2cm}] ([yshift=-.35cm]i.south) to node[below, yshift=-.2cm] {\tiny $n$} ([yshift=-.35cm]in.south);
			\draw[decorate, decoration={brace, mirror, amplitude=.2cm}] ([yshift=-.35cm]in.south) to node[below, yshift=-.2cm] {\tiny $n''$} ([yshift=-.35cm]right.south);
			\draw[decorate, decoration={brace, amplitude=.2cm}] ([yshift=.2cm]left.north) to node[above, yshift=.2cm] {\tiny $n$} ([yshift=.2cm]middle.north);
			\draw[decorate, decoration={brace, amplitude=.2cm}] ([yshift=.2cm]middle.north) to node[above, yshift=.2cm] {\tiny $n$} ([yshift=.2cm]right.north);
		\end{tikzpicture}
	\end{center}
	con $n' + n'' = n$ e $w_i \neq w_{i + n}$.
	Più precisamete l'automa segue le seguenti fasi
	\begin{enumerate}
		\item sulla pila viene contato $n'$.
		\item si sceglie nondeterministicamente un terminale $\sigma$ e si iniziano a togliere i $n'$ simboli accumulati finché non si svuota la pila.
		\item a questo punto si ricomincia a contare sulla pila fino a che non si sceglie un secondo terminale $\rho$ che deve essere differente.
		\item si ricomincia a togliere i simboli $n''$ dalla pila consumando l'input
		\item deve valere che alla fine dell'input sia vuota anche la pila e viceversa, affinché $n' + n'' = n$
	\end{enumerate}
\end{tcolorbox}

\begin{lemma}
	I DCFL sono chiusi rispetto al complemento.
\end{lemma}
Uno dei problemi con gli automi a pila è che questi possono non terminare.
Infatti utillizzando le $\varepsilon$-mosse l'automa può continuare a far oscillare la pila o far crescere la pila all'infinito.
Visto però che gli stati e i simboli della pila sono finiti, data una configurazione di superficie (\textit{surface configuration}, o lo stato e il simbolo in cima alla pila) capire se ci sarà un ciclo infinito è decidibile, e consiste banalmente nel controllare se lo stesso stato e simbolo di pila si ripetono senza consumare input.

Inoltre visto che sono ammesse le $\varepsilon$-mosse, in un automa che accetta per stato finale, può accadere che una volta che l'input è finito l'automa continui a fare mosse, e addirittura può finire in un ciclo infinito.
In ogni caso se c'è almeno uno stato finale tra quelli che visita dopo la fine dell'input, allora l'input è accettato.

\begin{lemma}
	Supponiamo che
	$$ M = \langle Q, \Sigma, \Gamma, \delta, q_0, Z_0, F \rangle $$
	sia un automa a pila deterministico.
	Costruiamo 
	$$ M' = \langle Q', \Sigma, \Gamma', \delta', q_0', X_0, F' \rangle $$
	tale che
	$$ L(M) = L(M') $$
	ed $M'$ scandisce sempre l'intero input.
\end{lemma}
\begin{proof}
	Definiamo l'automa $M'$ tale che
	\begin{itemize}
		\item $Q' = Q \cup \{q_0', d, f \}$, con $d$ detto \textit{stato trappola}
		\item $\Gamma' = \Gamma \cup \{X_0\} $
		\item $F' = F \cup \{ f \} $
	\end{itemize}
	e definiamo le mosse che fa l'automa
	\begin{itemize}
		\item $\delta'(q_0', \varepsilon, X_0) = (q_0, Z_0X_0)$, cioè -- come in altre costruzioni -- ``infiliamo'' in fondo alla pila un nostro simbolo.
			Questo è necessario perché la prossima mossa potrebbe non essere definita perché si è svuotata la pila.
		\item mostriamo ora alcune regole particolari che portano allo stato trappola
			\begin{itemize}
				\item se in una certa configurazione di superficie dell'automa $M$ non esiste una prossima mossa definita, cioè se
					$$\delta(q, a, X) = \delta(q, \varepsilon, X) = \varnothing, \hspace{1cm} q \in Q, a \in \Sigma, X \in \Gamma$$
					allora nell'automa $M'$ finisco nello stato trappola
					$$\delta'(q, a, X) = (d, X)$$
				\item se nell'automa $M$ la pila si è svuotata e non potrei pià fare mosse nell'automa $M'$ finisco nello stato trappola
					$$\delta'(q, a, X_0) = (d, X_0),\hspace{1cm}q \in Q', a \in \Sigma$$
				\item nello stato trappola consumo l'input e rimango nello stato trappola
					$$\delta'(d, a, X) = (d, X),\hspace{1cm}a \in \Sigma, X \in \Gamma'$$
				\item se da $(q, X)$, con $q \in Q$ e $X \in \Gamma$ è possibile un loop di $\varepsilon$-mosse
					$$\delta'(q, \varepsilon, X) = 
					\begin{cases}
						(d, X) & \text{se il loop non visita stati finali} \\
						(f, X) & \text{altrimenti}
					\end{cases}
					$$
				\item se sono nello stato finale $f$ e posso ancora leggere input, allora non sono alla fine quindi mi sposto nello stato trappola
					$$\delta'(f, a, X) = (d, X), \hspace{1cm}a \in \Sigma, X \in \Gamma'$$
			\end{itemize}
		\item in tutti gli altri casi $\delta'(q, a, X) = \delta(q, a, X)$ con $q \in Q, a \in \Sigma \cup \{\varepsilon\}, X \in \Gamma$
	\end{itemize}
\end{proof}

\begin{proof}[Automa per il complemento]
Sia 
$$ M = \langle Q, \Sigma, \Gamma, \delta, q_0, Z_0, F \rangle $$
un DPDA che accetta $L$ e scandisce sempre l'intero input.
Costruiamo l'automa per il complemento
$$ M' = \langle Q', \Sigma, \Gamma, \delta', q_0', Z_0, F' \rangle $$
con 
\begin{itemize}
	\item $Q' = Q \times \{y, n, A \}$, dove $y, n, A$ servono a ricordare se nell'ultima sequenza di $\varepsilon$-mosse ho visto uno stato finale, rispettivamente:
		\begin{itemize}
			\item $y$ indica che nell'attuale stato della sequenza di $\varepsilon$-mosse è stato visitato uno stato finale.
			\item $n$ indica che nell'attuale stato della sequenza di $\varepsilon$-mosse non è stato visitato uno stato finale.
			\item $A$ indica che non ho visitato uno stato finale e non posso fare alcuna mossa.
		\end{itemize}
	\item $F' = Q \times \{ A \}$, cioè tutti gli stati in cui non posso procedere e non ho visitato uno stato finale
\end{itemize}
Definiamo ora $\delta'$ come
\begin{itemize}
	\item se $\delta(q, \varepsilon, X) = (p, \gamma)$ allora 
		\begin{itemize}
			\item $\delta'([q, y], \varepsilon, X) = ([p, y], \gamma)$, quindi se in passato ho visitato uno stato finale, continuo a ricordare che ho visitato uno stato finale.
			\item se non ho ancora visto uno stato finale, e il prossimo lo è, cambio $n$ a $y$ 
				$$\delta'([q, n], \varepsilon, X) = \begin{cases} ([p, n], \gamma) & \text{se } p \not \in F \\ ([p, y], \gamma) & \text{se } p \in F \end{cases}$$
		\end{itemize}
	\item se $\delta(q, a, X) = (p, \gamma)$ con $a \in \Sigma$, allora
		\begin{itemize}
			\item se ero in uno stato $y$ e consumo dell'input e finisco in uno stato non finale, cambio $y$ a $n$
				$$\delta'([q, y], a, X) = \begin{cases} ([p, n], \gamma) & \text{se } p \not \in F \\ ([p, y], \gamma) & \text{se } p \in F \end{cases}$$
			\item alternativametne se ero in uno stato $n$ spezzo la mossa in due parti
				\begin{align*}
					\delta'([q, n], \varepsilon, X) &= ([q, A], X) \\
					\delta'([q, A], a, X) &= \begin{cases} ([p, n], \gamma) & \text{se } p \not \in F \\ ([p, y], \gamma) & \text{se } p \in F \end{cases}
				\end{align*}
				cioè prima di consumare dell'input assumo di aver finito la stringa senza esser passato per stati finali nella sequenza di $\varepsilon$-mosse.
				Se c'è ancora in input trasformo lo $A$ in $y$ o $n$.
		\end{itemize}
\end{itemize}
Infine definiamo
$$ q_0' = \begin{cases} [q_0, n] & \text{se } q_0 \not \in F \\ [q_0, y] & \text{se } q_0 \in F \end{cases} $$
\end{proof}

Dalla costruzione di sopra si vede che le $\varepsilon$-mosse sono molto fastidiose.
Come abbiamo visto attraverso la trasformazione in GNF nel caso di PDA si può fare a meno delle $\varepsilon$-mosse a patto di sacrificare la parola vuota.
Lo stesso non vale nel caso di DPDA, e senza $\varepsilon$-mosse otteniamo un modello meno potente.

Dai linguaggi 
\begin{align*}
	L' &= \{ a^i b^j c^k \mid i = j \} \\
	L'' &= \{ a^i b^j c^k \mid j = k \}
\end{align*}
Definiamo il linguaggio 
$$ L_0 = L' \cup dL'' \in DCFL $$
in base a se la stringa inizia con una $d$ o no sappiamo se dobbiamo riconoscere una parola di $L'$ o di $L''$.

\paragraph{Prodotto} Non deterministicamente quando un automa arriva in fondo faccio partire l'automa successivo.
Questo si può fare anche in termini di grammatiche, date $G' = \langle V', \Sigma, P', S' \rangle$ e $G'' = \langle V'', \Sigma, P'', S'' \rangle$ con $V' \cap V'' = \varnothing$ creiamo
$$ G = \langle V' \cup V'' \cup \{ S \}, \Sigma, P' \cup P'' \cup \{ S \rightarrow S' S'' \}, S \rangle $$
Per il caso deterministico utilizziamo $L_0$ di prima % ref
e definiamo
$$ L' = \{ \varepsilon, d \} \cdot L_0 = \{ d^s a^i b^j c^k \mid 0 \leq s \leq 2 \} $$
con 
$$\begin{cases} s = 0 & i = j \\ s = 2 & j = k \\ s = 1 & i = j \vee j = k \end{cases}\} $$
Per mostrare che è ambiguo prendiamo
$$ L' \cap d a^* b^* c^* = \{ d a^i b^j c^k \mid i = j \vee j = k \} \not \in \text{DCFL}$$
con $d a^* b^* c^*$ regolare, visto che i deterministici sono chiusi rispetto all'intersezione con regolari, $L'$ non può essere deterministico.
Inoltre $\{\varepsilon, d\}$ è finito, quindi non solo i linguaggi deterministici non sono chiusi rispetto al prodotto, ma non sono chiusi neanche rispetto al prodotto a sinistra con linguaggi finiti.

Si può mostrare però che i DCFL sono chiusi rispetto al prodotto a destra con regolari, cioè dati $L \in \text{DCFL}$ e $R \in \text{Reg}$ il prodotto $L \cdot R \in \text{DCFL}$.
Questa costruzione è simile al prodotto per gli automi a stati finiti.

\paragraph{Chiusura di Kleene -- star} Questo costruzione è molto simile al prodotto. 
Data la grammatica $G' = \langle V', \Sigma, P', S' \rangle$, costruiamo
$$ G = \langle V' \cup \{ S \}, \Sigma, P' \cup \{ S \rightarrow \varepsilon, S \rightarrow S' S \}, S \rangle $$
Nel caso dei deterministici prendiamo ancora $L_0$ e analizziamo
$$ L_0^* \cap d a^* b^* c^* = d (L_1 \cup L_2) $$
infatti questo ha stringhe della forma
$$ da^ib^jc^k $$
tali che
\begin{itemize}
	\item $dL_2$, per cui $j = k$
	\item o il prodotto $d \in L_2$ per $a^i b^j c^j \in L_1$, per cui $i = j$
\end{itemize}
ma questo abbiamo visto che non è deterministico, quindi $L_0^*$ non è chiuso rispetto alla star.

\paragraph{Morfismo} Per ogni terminale $a \in \Sigma$, lo sostituisco con una stringa $w \in \Delta^*$ utilizzando una funzione
$$ h : \Sigma \rightarrow \Delta^* $$
Per questo basta sostituire i terminali in ogni produzione.

Nel caso dei deterministici  prendiamo 
\begin{align*}
	L' &= \{ a^i b^j c^k \mid i = j \} \\
	L'' &= \{ a^i b^j c^k \mid j = k \} 
\end{align*}
e sappiamo che $L', L'' \in \text{DCFL}$, e che $L' \cup L'' \not \in \text{DCFL}$.
Mentre vale che $L_0 = L' \cup dL'' \in \text{DCFL}$.
Prendiamo il morfismo
$$ h(\sigma) = \begin{cases} \sigma & \text{se } \sigma \neq d \\ \varepsilon & \text{altrimenti} \end{cases} $$
vale che $h(L_0) = L' \cup L'' \not \in \text{DCFL}$, quindi i determinisici non sono chiusi rispetto al morfismo.

\paragraph{Sostituzione} Ad ogni terminale associamo un linguaggio, utilizzando una funzione
$$ s : \Sigma \rightarrow 2^{\Delta^*} $$
Se $L \in \text{CFL}$ e $\forall a \in \Sigma \mid s(a) \in \text{CFL}$, allora $s(L) \in \text{CFL}$.
Molto ad alto livello ad ogni terminale corrisponde una grammatica, nelle produzioni sostituiamo ai terminali l'assioma della grammatica.

Visto che il morfismo è un caso particolare della sostituzione, i determinisitici non sono chiusi rispetto alla sostituzione.

% lezione 19
% !!! MISSING !!!
% lezione del 15-05-2024

% lezione 20
Data la grammatica
$$ G = \langle V = \{S, T, A, B\}, \Sigma = \{a, b, c, d\}, S, P \rangle $$
con $P$ definito come
\begin{align*}
		S &\rightarrow cAT \mid c T \\
		T &\rightarrow  d S \mid d \\
		A &\rightarrow B a A \mid B a \\
		B &\rightarrow b B \mid \varepsilon
\end{align*}
% ????
% !!! MISSING !!!
\begin{align*}
	S &\rightarrow c A T \mid c T \\
	T &\rightarrow A S \mid d 
\end{align*}

% fig 20.1
\begin{center}	%TODO: inintelligible ****
	\begin{tikzpicture}
		\node[state, initial] (s) {$S$};
		\node[state] (t) [right=of s] {$T$};
		\node[state] (q) [above right=of s] {$q$};
		\node[state, accepting] (f) [below right=of s] {};

		\draw[->] (s) edge node[above left] {$C$} (q);
		\draw[->] (s) edge[bend left] node[above] {$C$} (t);
		\draw[->] (q) edge node[above right] {$A$} (t);
		\draw[->] (t) edge[bend left] node[below] {$D$} (s);
		\draw[->] (t) edge[bend left] node[below right] {$D$} (f);
	\end{tikzpicture}
\end{center}

Proviamo a fare la conversione da automa a linguaggio regolare:
$$ \left \{ \begin{array}{ccc} S & = & c Q + c T \\ Q & = & A T \\ T & = & d S + d \end{array} \right .$$
sostituendo la terza nella prima e la seconda abbiamo
$$ \left \{ \begin{array}{ccc} S & = & c Q + c d S + c d \\ Q & = & A d S + A d \\ T & = & d S + d \end{array} \right .$$
e poi sostituiamo la seconda nella prima
$$ \left \{ \begin{array}{ccc} S & = & c A d S + c A d + c d S + c d \\ Q & = & A d S + A d \\ T & = & d S + d \end{array} \right .$$
e semplificando
$$ \left \{ \begin{array}{ccc} S & = & (c A d + c d)S + c A d + c d \\ Q & = & A d S + A d \\ T & = & d S + d \end{array} \right .$$
che corrisponde a $S = (cAd + cd)^* (cAd + c) = (cAd + cd)^+$.

Alla regola $A \rightarrow B a A \mid B a$ corrisponde il linguaggio regolare $A \overset{*}{\Rightarrow} (B a)^+$, e a $B \rightarrow b B \mid \varepsilon$ corrisponde ovviamente $B \overset{*}{\Rightarrow} b^*$.
Quindi sostituendo nel regolare di $A$ abbiamo $A \overset{*}{\Rightarrow} (b^* a)^+$, e sostituendo in $S \overset{*}{\Rightarrow} (c (b^* a)^+ d + cd)^+$.
Quindi $L(G) = (c (b^* a)^* d)^+$.

\begin{teorema}[Teorema di Chomsky-Schutzenberger]	% sciutzenberger, pronunciabile alla francese (bergie) o alla tedesca (bergher)
	Sia $\Omega_k$ un alfabeto di $k$ tipi di parentesi
	$$ \Omega_k = \{ (_1, (_2, \dots, (_k, )_1, )_2, \dots, )_k \} $$
	$D_k \subseteq \Omega_k^*$ è detto un linguaggio di Dyck, se è il linguaggio delle parentesi bilanciate correttamente.
	Sia $L \subseteq \Sigma^*$ un linguaggio CF, allora 
	$$ \exists k > 0 \mid h : \Omega_k \rightarrow \Sigma \; \text{morfismo} \; \text{e} R \subseteq \Omega_k \; \text{regolare} $$
	tale che $L = h(D_k \cap R)$.
\end{teorema}
Non dimostrato.

\begin{tcolorbox}[breakable]
	Prendiamo il caso banale $L = D_1$, allora $k = 1$ e $h = \text{id}$ e $R = \Omega_1^*$.
\end{tcolorbox}
\begin{tcolorbox}[breakable]
	Prendiamo il caso 
	$$L = \{ a^n b^n \mid n \geq 0 \} $$
	allora $k = 1$ e 
	$$ h = \begin{cases} ( = a \\ ) = b \end{cases} $$ 
	e infine $R = (^* )^*$.
\end{tcolorbox}

\begin{tcolorbox}[breakable]
	Prendiamo il caso 
	$$L = \{ w w^R \mid \mid w \in \{a, b\}^* \} $$
	allora prendiamo $k = 2$ e 
	$$ h = \begin{cases} ( = a & \\ a = ) \\ [ = b \\ ] = b \end{cases} $$ 
	e infine il filtro $R = (\text{\texttt{$($}} + \text{\texttt{$[$}})^* (\text{\texttt{$]$}} + \text{\texttt{$)$}})^*$.
\end{tcolorbox}

\begin{tcolorbox}[breakable]
	Prendiamo il caso 
	$$L = \{ x \mid \mid x \in \{a, b\}^* \wedge x = x^R \} $$
	allora prendiamo $k = 4$ e similmente a prima definiamo $h$
	$$ h = 
	\begin{cases} 
		\langle = a \\
		\rangle = \varepsilon \\
		\lfloor = b \\
		\rfloor = \varepsilon \\
		( = a \\ 
		a = ) \\ 
		[ = b \\ 
		] = b 
	\end{cases} $$ 
	e infine il filtro $R = (\text{\texttt{$($}} + \text{\texttt{$[$}})^* ( \varepsilon + \text{\texttt{$\langle \rangle$}} + \text{\texttt{$\lfloor \rfloor$}}) (\text{\texttt{$]$}} + \text{\texttt{$)$}})^*$.
\end{tcolorbox}
Quindi il riconoscimento di un linguaggio CF si riconduce a riconoscere un linguaggio di parentesi.
Quindi si potrebbe pensare di costruire una macchina
% fig 20.2
Il componente $h^{-1}$ può essere realizzato come un componente a stati finiti.
Quindi l'unica cosa non a stati finiti è il componente $D_k$.

\section{Lingauaggi unari}\label{sec:linguaggi unari}
Un linguaggio unario è un linguaggio tale che $| \Sigma | = 1$.
Mostreremo che nel caso unario i linguaggi CF e i regolari sono la stessa classe.

Supponiao di avere un automa deterministico per un alfabeto unario
% fig 20.3
Visto che c'è un solo simbolo, questi sono formati da una serie di stati e opzionali loop indietro.
Il linguaggio riconosciuto da sopra è
$$ L = \varepsilon, a^6, a^11, a^16, \dots = a^0 + a^6(a^5)^* $$
Modificandolo in questo modo
% fig 20.4
$$ L = a^0 + a^5(a^6)^* + a^6(a^5)^*$$
Questi rappresentano più o meno successioni numeriche.

Prendiamo ora l'automa
% fig 20.5
questo riconosce
$$ L = a^8 + a^{11}(a^5)^* + a^{12}(a^5)^* $$
% ???? mi sono perso

\begin{lemma}
	Supponiamo di avere una serie di naturali
	$$ l_1, l_2, l_3, \dots $$
	e un $q > 0$.
	Supponiamo di avere linguaggi del tipo
	$$ K_i = a^{l_i} (a^q)^* $$
	allora l'unione
	$$ \bigcup_{i \geq 0} K_i \in \text{Reg} $$
\end{lemma}
Noi abbiamo visto che l'unione di linguaggi regolari è ancora regolare, ma qui il risultato importante è che l'unione può essere di un numero potenzialmente infinito di regolari.il risultato importante è che l'unione può essere di un numero potenzialmente infinito di regolari.
Ad esempio questo non vale per l'unione infinita di 
$$ \bigcup_{i \geq 0} a^i b^i $$

Tutti questi possono essere ricondotti all'automa di sopra purché abbiano tutti lo stesso periodo $q$.

\begin{lemma}
	Sia $N$ un intero e prendiamo dei linguaggi
	$$ K_i = a^{l_i} (a^{q_i})^* \subseteq a^* $$
	quindi senza un periodo fissato.
	Se però che $\forall i \mid 1 \leq q_i \leq N$, allora l'unione
	$$ \bigcup_i K_i \in \text{Reg} $$
\end{lemma}
Infatti possiamo generare per il lemma di prima i linguaggi regolari per ogni $q$, e visto che $N$ è finito facciamo l'unione finita di tutti i linguaggi per ogni $q$ nel range.

\begin{teorema}
	$L \subseteq a^* \in \text{CFL}$ implica $L$ regolare.
\end{teorema}
\begin{proof}
	Utilizzeremo il pumping lemma per i linguaggi CF.
	Sia $N$ la costante del pumping lemma per $L$.
	Prendiamo $a^m \in L$ con $m \geq N$, allora
	$$ a^m = u v w x y $$
	sappiamo che
	\begin{itemize}
		\item $vx \neq \varepsilon$, e chiamiamo $q_m = |vx|$
		\item $|vwx| \leq N$
		\item $\forall i \geq 0 \mid u v^i w x^i y \in L$, la lunghezza di questa stringa è (escludendo il caso $i = 0$) 
			$$ m + (i - 1)q^m $$
			quindi $a^{m + (i - 1)q_m} \in L $
			cioè $a^m (a^{q_m})^* \subseteq L$
	\end{itemize}
	quindi $q \leq q_m \leq N$.
	Prendiamo
	\begin{align*}
		L' &= \{ a^m \in L \mid m < N \} \\
		L'' &= \{ a^m \in L \mid m \leq N \} 
	\end{align*}
	e vale che
	$$ L = L' \cup L'' \subseteq L' \cup \bigcup_{m \geq N} a^m (a^{q_m})^* \subseteq L $$
	quindi
	$$ L = L' \cup \bigcup_{m \geq N} a^m(a^{q_m})^* $$
	e $L'$ è regolare perché finito e per il lemma di prima anche il secondo è ancora regolare, visto che $1 \leq q_m \leq N$ per il pumping lemma.
	E visto che l'unione di due regolari è regolare, allora $L$ è regolare.
\end{proof}
Dati 
% fig 20.7
si può vedere che generalmente il periodo è il $\text{MCD}$ tra i due periodi dei linguaggi.

Nei linguaggi unari si può mostrare lo strano risultato che per passare da un nodeterministico a un deterministico il costo è $e^{\sqrt{n \ln{n}}}$

\begin{teorema}[Teorema di Parikh]
	Dato un alfabeto $\Sigma$ di qualsiasi dimensione e una stringa $w$.
	Sia ad esempio $\Sigma = \{a, b\}$, associamo a questa stringa il vettore 
	$$ \phi(w) = ( \#_a(w), \#_b(w) ) $$
	chiamato immagine di Parikh.
	E definiamo l'immagine di Parikh del linguaggio come
	$$ \phi(L) = \{ \phi(w) \mid w \in L \} $$
	Se $L \in \text{CFL}$, allora $\exists L' \in \text{Reg}$ tale che $\phi(L) = \phi(L')$.
\end{teorema}
Quindi se non ci interessa l'ordine dei simboli, il linguaggi regolari e i linguaggi CF sono uguali.
Un corollario è che vale l'uguaglianza del CF e dei regolari nel caso di alfabeti unari.

\begin{tcolorbox}[breakable]
	Sia 
	$$ L = \{ a^n b^n \mid n \geq 0 \} $$
	e 
	$$ \phi(L) = \{ (n, n) \mid n \geq 0 \} $$
	allora possiamo prendere il linguaggio regolare $R = (ab)^*$ e vale che
	$$ \phi(L) = \phi(R) $$
\end{tcolorbox}

% lezione 21
\section{Automi a pila two-way}\label{sec:Automi a pila two-way}
Degli automi a pila abbiamo studiato un modello one-way.
A differenza degli FSA già nel modello one-way abbiamo differenza tra il modello detrministico e non-deterministico.

Vediamo ora il modello two-way, e prendiamo il linguaggio
$$ L = \{ a^n b^n c^n \mid n \geq 0 \} $$
nel caso one-way way l'informazione del numero di $a$ viene distrutta durante il confronto con le $b$.
Mentre nel caso two-way possiamo tornare indietro a recuperarla per confrontare le $a$ anche con le $c$.
Anche il caso 
$$ L = \{ w \# w \mid w \in \{a, b\}^* \} $$
basta copiare $w$ sulla pila fino al cancelletto, poi si salta fino in fondo e si confronta con la seconda stringa.
% Supponendo di togliere il cancelletto non 

Pendiamo un altro linguaggio
$$ L = \{ a^{2^n} \mid n \geq 0 \} $$
Dobbiamo ``dividere per due'' iterativametne.
Questo è fatto scorrendo la stringa e ogni due simboli aggiungendone uno sulla stringa.
Ad esempio
% fig 21.1
In questo modo se arriviamo all'end marker in uno stato ``dispari'', è equivalente ad avere resto non zero e quindi non si accetta (tranne nel caso $2^0$).
Quindi è possibile riconoscere questo linguaggio, che non è CF.

Ritornando all'esempio di prima
$$ L = \{ w w \mid w \in \{a, b\}^* \} $$
con il sistema di prima riusciamo a trovare la metà con la pila vuota.
Una volta a metò si carica la seconda parte della stringa sulla pila.
Poi si ritrova la metà e si confronta con la prima parte della stringa al cotnratio.
% fig 21.2
Ed è per giunta deterministico.

Chiamiamo i PDA (deterministici e non ) two-way 2PDA, mentre i deterministici li chiamiamo 2DPDA.
Abbiamo visto che sicuramente sono più potentei dei PDA normali perché abbiamo visto 3 linguaggio non CF che riescono a riconoscere.
Non si sa però se 2PDA e 2DPDA siano classi distinti, cioè se il modello nondeterministico sia più potente di quello determnistico.

Inoltre sicuramente non si sà se la classe dei 2PDA sia strattamente inclusa nei CS, se sia più grande, o se siano inconfrontabili.

Inoltre non si sà se i linguaggi riconosciuti dai 2DPDA siano un superset dei CFL.

\section{Automi limitati linearmente}\label{sec:automi limitati linearmente}
Supponiamo ora di avere un automa two-way che può anche modificare il nastro.
Questo è chiamato automa limitato linearmente, o LBA.

\begin{tcolorbox}[breakable]
	Il linguaggio
	$$ L = \{ a^n b^n c^n \mid n \geq 0 \} $$
	è riconoscibile cancellando la prima $a$, poi la prima $b$ e poi la prima $c$ e poi tonando indietro all'inizio della stringa.
	Si procede in questo modo finché non finiscono le $a$ e se il nastro è tutto cancellato allora la parola è accettata.
\end{tcolorbox}
\begin{tcolorbox}[breakable]
	Il linguaggio
	$$ L = \{ a^{2^n} \mid n \geq 0 \} $$
	è riconosciuto similmente a prima: ogni due $a$ se ne cancella una, una volta arrivati alla fine si torna all'inizio e si ripete.
\end{tcolorbox}
\begin{tcolorbox}[breakable]
	Per il linguaggio
	$$ L = \{ w w \mid w \in \{a, b\}^* \} $$
	si trova il centro
	% fig 21.3
	\begin{center}
		\begin{tikzpicture}
			\begin{scope}[local bounding box=lineBox1, start chain=line1, node distance=0pt, name prefix=l1-]
    				\node [on chain=line1] {$\rhd$};
				\node [on chain=line1] (n1) {$a$};
				\node [on chain=line1] (n2) {$b$};
				\node [on chain=line1] (n2) {$b$};
				\node [on chain=line1] (n3) {$b$};
				\node [on chain=line1] (n4) {$a$};
				\node [on chain=line1] (n5) {$b$};
				\node [on chain=line1] (n6) {$b$};
				\node [on chain=line1] (n7) {$b$};
				\node [on chain=line1] {$\lhd$};
 			\end{scope}
		\end{tikzpicture}
	\end{center}
	una volta trovata la metà il simbolo che la si ricorda nello stato e lo si cancella.
	si va fino in fondo e si cerca se l'ultimo simbolo è uguale e se lo è lo si cancella e si va avanti.
\end{tcolorbox}

\begin{tcolorbox}[breakable]
	Per il linguaggio delle parentesi a un simbolo.
	Si cerca la prima parentesi chiusa e si controlla se il primo simbolo precedente (non cancellato) è una aperta.
	Si procede iterativamente finché non si arriva all'end-marker.
	\begin{align*}
		\rhd ( ( ) ( ) ) (()) \lhd \\
		\rhd ( \cancel{(} \cancel{)} ( ) ) (()) \lhd \\ % ...
	\end{align*}
\end{tcolorbox}

\begin{tcolorbox}[breakable]
	Vediamo il linguaggio di quadrati di interi in notazione unaria
	$$ L = \{ a^{n^2} \mid n \geq 0 \} $$
	possiamo usare il fatto che il quadrato di $n$ è uguale alla somma dei primi $n$ numeri dispari
	$$ n^2 = \sum_{i = 1}^n (2i + 1) $$
	Ora procediamo le seguente modo
	\begin{align*}
		\rhd aaaaaaaaaaaaaaaa \lhd \\
		\rhd Xaaaaaaaaaaaaaaa \lhd \\
		\rhd XYYYaaaaaaaaaaaa \lhd \\
		\rhd XYYYXXXXXaaaaaaa \lhd \\
		\rhd XYYYXXXXXYYYYYYY \lhd \\
	\end{align*}
	Quindi l'idea per un blocco è fare la copia del blocco precedente e aggiungere 2.
\end{tcolorbox}

Gli automi LBA (non deterministici) corrispondono alla classe dei linguaggi CS.
Ricordiamo che le grammatice CS sono della forma
$$ \alpha \rightarrow \beta, \hspace{1cm} |\beta| \geq |\alpha | $$
L'idea dell'automa è di simulare la derivazione applicando le regole al contrario: se si trova una parte dell'input che corrisponde ad una parte destra di una regola, lo si sostituisce con la sua parte sinistra (nel caso riepmpiendo lo spazio extra con un simbolo nullo).

In generale si può mostrare che i linguaggi CS possono essere riconosciuti anche da macchine che hanno un nastro di input e un nastro di lavoro separato (di lunghezza limitata).
Infatti i linguaggi CS sono riconosciuti da tutte queste macchine che hanno una memoria limitata linearmente rispetto l'input.

Vediamo ora le proprietà di chiusura di questi linguaggi:
\begin{itemize}
	\item chiusi rispetto all'unione, il motivo è lo stesso della chiusura rispetto all'unione dei CF
	\item chiusi rispetto all'intersezione: si esegue il primo automa e se questo accetta si esegue il secondo.
		Per evitare che il primo automa sporchi l'input per il secondo, si ``sdoppia'' l'input e ogni cella contiene una coppia di simboli e il primo automa tocca solo uno dei due elementi.
	\item chiusi rispetto al complemento (dimostrato da Immermann nell'88).
		Se c'è una macchina nondeterministica che lavora in spazio $s(n)$ della lunghezza di input, si può ottenre una macchina limitata da $s(n)$ per il complemento
\end{itemize}
Non si sà se il modello nondeterministico sia più potente o no di quello deterministico
$$ \text{DLBA} \overset{?}{=} LBA $$
\begin{teorema}[Teorema di Sevitch]
	Se ho una classe di linguaggi riconosciuta da una macchina nondeterministica, si può sempre costruire una macchina determinisitca che riconosce in spazio quadratico.
	$$ \text{NSPACE}(s(n)) \subseteq \text{DSPACE}(s^2(n)) $$
\end{teorema}
Quindi da questo 
$$ \text{CS} = \text{NSPACE}(n) \subseteq \text{DSPACE}(n^2) $$
ma non si sà se si può fare meglio di così.

\section{Problemi di decisione sui linguaggi CF}\label{sec:problemi di decisione sui linguaggi CF}
\begin{teorema}
	Sia $L \in \text{CF}$ e $N$ la sua costante del pumping lemma per $L$, allora
	$$ \begin{cases}
		L \neq \varnothing & \text{sse } \exists w \in L \mid |w| < N \\
		L \text{ è infinito} & \text{sse } \exists w \in L \mid N \leq |w| < 2N
	\end{cases}
	$$
\end{teorema}
La dimsotrazione di questo è uguale allo stesso teorema per i regolari.

Per mostrare che $L \neq \varnothing$, supponiamo che $L$ sia generato dalla grammatica
$$ G = \langle V, \Sigma, P, S \rangle $$
Possiamo costrire l'insieme delle variabili che sono in grado di generare terminali, e se questo contiene $S$ allora il linguaggio non è vuoto.
Iniziamo costruendo l'insieme
$$ T_0 = \Sigma $$
e 
$$ T_{i + 1} = T_i \cup \{ A \in V \mid \exists A \rightarrow B \in P \mid \beta \in T_i^* \} $$
Vale che
$$ T_0 \subseteq T_1 \subseteq \dots \subseteq \Sigma \cup V $$

Mostriamo ora se un linguaggio $L$ è finito o infinito.
Supponiamo che $L$ sia riconosciuto dalla grammatica $G$ in FNC, priva di simboli raggiungibili e produttivi.
Costruiamo il grafo delle produzioni: i vertici sono le variabili ed esiste un arco da $A$ a $B$ se esiste una variabile $C$ tale che
$$ A \rightarrow BC \in P \vee A \rightarrow CB \in P $$
Dato questo grafo, $L$ è infinito sse il grafo contiene un ciclo.
Infatti vuol dire che esiste una serie di produzioni per cui una variabile può riprodurre sé stessa e visto che tutte le variabili in FNC sono produttive, la stringa non può che crescere.

Quindi questa è una quesione decidibile.

Vedremo invece che non possiamo decidere se 
$$ L \overset{?}{=} \Sigma^* $$


% lezione 22
Si possono considerare modelli diversi rispetto ai LBA, ad esempio un automa con un nastro di input read-only e con $n$-nastri ausiliari.

\begin{definizione}[Memoria di Turing ad un nastro]
	Un macchina di Turing è un automa con una testina e un nastro illimitato che, sulle celle che non contengono l'input, contiene un carattere speciale detto \textit{blank}.
	$$ M = \langle Q, \Sigma, \Gamma, \delta, q_0, F \rangle $$
	\begin{itemize}
		\item $Q$ un insieme di stati
		\item $\Sigma$ un alfabeto, e $\Sigma \subseteq \Gamma$
		\item $\Gamma$ un alfabeto di lavoro, e il blank $\# \in \Gamma \setminus \Sigma$
		\item $q_0 \in Q$
		\item $F \subseteq Q$
		\item $\delta : Q \times \Gamma \rightarrow Q \times \Gamma \times \{-1, 0, +1\} $ nel caso determinsitico,
			dove
			\begin{itemize}
				\item $-1$ è un movimento a destra
				\item $0$ nessun movimento
				\item $+1$ un movimento a destra
			\end{itemize}
	\end{itemize}
\end{definizione}
Quindi una macchina di Turing può utilizzare una memoria illimitata.

Ci sono diverse varianti dell'automa, ad esempio con un nastro infinito solo da un lato, o più nastri.

Assumeremo che gli stati finali siano stati \textit{halting}, quindi che non ci siano transizioni da essi, e inoltre assumeremo che il blank non possa essere scritto.

\`E importante notare che la macchina potrebbe entrare in un loop.

Nel caso delle macchine di Turing, il modello nondeterministico e il modello deterministico sono gli stessi, se non ci sono vincoli di risorse.
Infatti una macchina nondetermininistica in ogni momento può fare delle scelte, nella versione deterministica si utilizza una BFS.

Si può mostrare che la classe dei linguaggi riconosciuti da questa macchina sono i linguaggi di tipo 0.
Infatti si può pensare di, tenendo l'input sul nastro, e applicando tutte le produzioni al contrario finché non si trova l'assioma, opzionalmente andando avanti in eterno.
Possiamo avere quindi tre possibili risposte
% fig. 22.1
\begin{center}
	\begin{tikzpicture}
		\node (init) {};
		\node[draw, minimum width=1.5cm, minimum height=1cm] (xinL) [right=of init] {$x \in L$};
		\node[minimum width=.8cm] (si) [above right=of xinL] {Sì};
		\node[minimum width=.8cm] (no) [right=of xinL] {No};
		\node[minimum width=.8cm] (bot) [below right=of xinL] {$\bot$};

		\draw[->] (init.east) -- node[above] {$x$} (xinL.west);
		\draw[->] ([yshift=.4cm]xinL.east) -- (si.west);
		\draw[->] (xinL.east) -- (no.west);
		\draw[->] ([yshift=-.4cm]xinL.east) -- (bot.west);

		\draw[decorate, decoration={brace, amplitude=.2cm}] ([xshift=.1cm]no.east) to node[right, xshift=.4cm] {$x \not \in L$} ([xshift=.1cm]bot.east);
	\end{tikzpicture}
\end{center}
per questo i linguaggi sono detti ricorsivi numberabili.

Possiamo vedere le macchine di Turing anche come macchine che calcolano funzioni, e le funzioni calcolate da una macchina di Turing sono le stesse calcolate da un linguaggio di programmazione generico.

Vediamo ora alcuni problemi indecibili per le macchine di Turing.
\begin{itemize}
	\item problema dell'arresto (HALT), tale che abbiamo in input una macchina di Turing $M$ e una stringa $x \in \Sigma^*$, e ci chiediamo se la macchina termini su input $x$
	\item vuotezza del linguaggio riconosciuto, tale che abbiamo in input una macchina di Turing $M$ e ci chiediamo se $L(M) = \varnothing$
\end{itemize}

\begin{definizione}[Configurazione]
Definiamo ora la configurazione di una macchina di Turing.
Questa non contiene l'intero nastro infinito, ma solo la parte non-blank.
Suponiamo che la parte a sinistra della testina sia chiamata $x$ e che la parte della stringa da dove la testina si trova all'estremo destro si chiami $y$, una configurazione è
$$ xqy $$
con $q \in Q$ e $x, y \in \Gamma^*$.
\end{definizione}

\begin{definizione}[Configurazione iniziale]
	La configurazione iniziale su input $w$ è
	$$ q_0w $$
\end{definizione}
\begin{definizione}[Configurazione finale]
	Una configurazione è finale o acecttante se lo stato $q$ è finale.
\end{definizione}

\begin{definizione}[Mossa]
	Una mossa, scritta $C \vdash C'$, o nel caso di più mosse $C \overset{*}{\vdash*} C'$ è una applicazione di $\delta$.
\end{definizione}

\begin{definizione}[Linguaggio riconosciuto da una macchina di Turing]
	Data la definizione di mossa definiamo il linguaggio riconosciuto da una macchina di Turing $M$ come
	$$ L(M) = \{ w \in \Sigma^* \mid q_0w \overset{*}{\vdash} xqy, x, y \in \Gamma^*, q \in F \} $$
\end{definizione}
Supponiamo di trovarci nella configurazione
% fig 22.2.
\begin{center}
	\begin{tikzpicture}
		\begin{scope}[local bounding box=tapeBox, start chain=tape, node distance=0pt]
    			\node [minimum width=40pt, minimum height=20pt, on chain=tape] {$\dots$};
			\node [draw, minimum width=20pt, minimum height=20pt, on chain=tape] {$\#$};
			\node [draw, minimum width=20pt, minimum height=20pt, on chain=tape] {$\#$};
			\node [draw, minimum width=20pt, minimum height=20pt, on chain=tape] (n1) {};
			\node [draw, minimum width=20pt, minimum height=20pt, on chain=tape] (n2) {c};
			\node [draw, minimum width=20pt, minimum height=20pt, on chain=tape] (n3) {};
			\node [draw, minimum width=20pt, minimum height=20pt, on chain=tape] (n4) {a};
			\node [draw, minimum width=20pt, minimum height=20pt, on chain=tape] (n5) {};
			\node [draw, minimum width=20pt, minimum height=20pt, on chain=tape] {$\#$};
			\node [draw, minimum width=20pt, minimum height=20pt, on chain=tape] {$\#$};
    			\node [minimum width=40pt, minimum height=20pt, on chain=tape] {$\dots$};
 		\end{scope}
		\draw[decorate, decoration={brace, amplitude=.2cm}] ([yshift=-.1cm]n1.south east) to node[below, yshift=-.2cm] {\tiny $x'$} ([yshift=-.1cm]n1.south west);
		\draw[decorate, decoration={brace, amplitude=.2cm}] ([yshift=.1cm]n1.north west) to node[above, yshift=.2cm] {\tiny $x$} ([yshift=.1cm]n2.north east);
		\draw[decorate, decoration={brace, amplitude=.2cm}] ([yshift=-.1cm]n3.south east) to node[below, yshift=-.2cm] {\tiny $q$} ([yshift=-.1cm]n3.south west);
		\draw[decorate, decoration={brace, amplitude=.2cm}] ([yshift=.1cm]n3.north west) to node[above, yshift=.2cm] {\tiny $y$} ([yshift=.1cm]n5.north east);
		\draw[decorate, decoration={brace, amplitude=.2cm}] ([yshift=-.1cm]n5.south east) to node[below, yshift=-.2cm] {\tiny $y'$} ([yshift=-.1cm]n5.south west);
	\end{tikzpicture}
\end{center}
con $xqy = xqay'$.
possiamo avere tre casi di mossa $\delta(q, a)$
\begin{itemize}
	\item $(p, b, 0)$ fa passare alla configurazione $xqay' \vdash xpby'$
	\item $(p, b, +1)$ fa passare alla configurazione $xqay' \vdash xbpy'$
	\item $(p, b, -1)$ fa passare alla configurazione $xqay' \vdash x'pcby'$
\end{itemize}

Chiamiamo (questo cancelletto non è il blank, è un altro)
$$ \Delta = Q \cup \Gamma \cup \{\# \} $$
e definiamo un linguaggio su questo alfabeto
$$ L'_{succ} = \{ \alpha \# \beta^{R} \mid \text{$\alpha$ e $\beta$ sono configurazioni di $M$ e $\alpha \vdash \beta$} \} $$
Per riconoscere questo basta un automa a pila, infatti ad esempio sia $\alpha = xqay'$ e $\beta = y'^Rbpx^R$, carico $x$ e con la funzione di transizione calcolo con cosa sostituire $qa$ e carico $y'$ e poi scarico dopo il cancelletto.
Definiamo anche
$$ L''_{succ} = \{ \alpha^R \# \beta \mid \dots \} $$
come sopra.
Questi automi a pila sono deterministici, visto che il successore è deterministico.

Definiamo ora il linguaggio delle computazioni valide per un automa $M$, o le computazioni accettanti
$$ \text{valid}(M) = \{ \alpha_1 \# \alpha_2^R \# \alpha_3 \# \dots \# \alpha_k^{(R)} \mid \alpha_i \in (Q \cup \Gamma)^* \} $$
$(R)$ indica che se $k$ è dispari allora non è rovesciato, altrimenti sì.
E devono valere le seguenti condizioni:
\begin{itemize}
	\item per $i \in 1, \dots, k$ deve valere che $\alpha_i \in \Gamma^* Q \Gamma^*$, cioè $\alpha_i$ rappresenta una configurazione di $M$
	\item $\alpha_1$ rappresenta una configurazione iniziale
	\item $\alpha_k$ rappresenta una configurazione accettante
	\item per $i \in 2, \dots, k$, deve valere che $\alpha_{i - 1} \vdash \alpha_i$, cioè $\alpha_i$ è raggiunto in una mossa da $\alpha_{i - 1}$
\end{itemize}
La prima, la seconda e la terza condizioni possono essere controllate da automi a stati finiti.
Per la quarta condizione, abbiamo visto che un passaggio è il linguaggio $L'_{succ}$ (o $L''_{succ}$), ma confrontare dopo aver confrontato $\alpha_1$ e $\alpha_2^R$, abbiamo distrutto la pila per $\alpha_3$.
Quello che possiamo fare con un automa a pila è confrontare $\alpha_1$ e $\alpha_2^R$, $\alpha_3$ e $\alpha_4^R$ e così via.
Poi si può fare un secondo automa che confronta $\alpha_2^R$ e $\alpha_3$, $\alpha_4^R$ e $\alpha_5$, e così via.
Quindi questo linguaggio è l'intersezione di due linguaggi CF.

\begin{teorema}
	Esistono due linguaggi CF $L_1$ e $L_2$ tali che $\text{valid}(M) = L_1 \cap L_2$, ed esiste un algoritmo che data $M$ costruisce PDA (deterministico) o CFG per $L_1$ e $L_2$.
\end{teorema}
Se la macchina $M$ riconosce l'insieme vuoto, quindi
$$ L(M) = \varnothing $$
sse non ci sono computazioni valide, quindi
$$ \text{valid}(M) = \varnothing $$
sse $L_1 \cap L_2 = \varnothing$.
Quindi non è possibile decidiere se l'intersezione di due linguaggi CF è vuota, perché se si potesse allora potremmo anche decidere se un certo linguaggi riconosciuto da una macchina di Turing è vuota.
\begin{corollario}
	Non è possibile decidere se l'intersezione di due CFL (e anche DCFL) è vuota.
\end{corollario}

Vediamo ora invece
$$ ( \text{valid}(M) )^C = \Delta^* \setminus \text{valid}(M) $$
Costruiamo un dispositivo nondeterministico che scommette quele delle quattro condizioni non sia rispettata.
\begin{itemize}
	\item se la prima è infranta basta controllare che tra due cancelletti non ci sia uno stato, o ci siano più stati.
	\item ...
	\item ...
	\item esiste almeno un $i$ tale che $\alpha_i$ non è raggiunta da una mossa da $\alpha_{i - 1}$.
		Questo è verificabile con una pila.
\end{itemize}
Le prime proprietà sono controllabili ancora con automi a stati finiti.
Mentre la quarta è riconoscibile da una pila.
\begin{teorema}
	$(\text{valid}(M))^C \in \text{CFL}$ ed esiste un algoritmo che, data $M$ costruisce una CFG o un PDA per $(\text{valid}(M))^C$.
\end{teorema}
Vale che $L = \varnothing$ sse $\text{valid}(M) = \varnothing$ sse $(\text{valid}(M))^C = \Delta^*$.
Se esistesse un algoritmo per determinare se un $L \in \text{CFL} = \Delta^*$, allora potremmo determinare che $L(M) = \varnothing$.
\begin{corollario}[Problema dell'universalità]
	Dato $L \in \text{CFL}$ non è possibile determinare (è indecidibile) se $L = \Sigma^*$.
\end{corollario}
Invece visto che i DCFL sono chiusi rispetto al complemento, basta costruire l'automa a pila per un linguaggio e vedere se il suo complemento riconosce il linguaggio vuoto.
\begin{corollario}[Problema dell'equivalenza]
	Dati $L_1, L_2 \in \text{CFL}$ non è possibile decidere se $L_1 = L_2$.
\end{corollario}
Questo perché il problema dell'universalità è un caso particolare dell'equivalenza ($L_1 = \Delta^*$).

\begin{corollario}[Problema dell'equivalenza con regolari]
	Dati $L \in \text{CFL}$ e $L \in \text{Reg}$, non è possibile decidere se $L = R$.
\end{corollario}
La motivazione è la stessa di sopra.

Invece dati due DPDA è possibile determinare se sono equivalenti.

%%%
Un automa a pila con due pile ha la stessa potenza computazionale di una macchina di Turing.

% lezione 23
\begin{teorema}
	Esistono $L_1, L_2$ linguaggi CF deterministici, tali che
	$$ \text{valid}(M) = L_1 \cap L_2 $$
	ed esiste un algoritmo che, data la macchina $M$, ci costruisce due automi a pila deterministici per $L_1$ e $L_2$ (o grammatiche non ambigue).
\end{teorema}
\begin{corollario}
$L_1 \cap L_2 = \varnothing$ sse $\text{valid}(M) = \varnothing$ sse $L(M) = \varnothing$.
Visto che non è possibile capire se un linguaggio di tipo 0 è vuoto, non è decidibile sapere se l'intersezione di due linguaggi CF (anche determinisitic) è vuota.
\end{corollario}

Se invece analizziamo $L_1 \cup L_2$, questo contiene sia computazioni valide che non.
Prendiamo $\beta \in \Delta^*$, e supponiamo che $\beta \in L_1 \cup L_2$.
Definiamo allora il PDA $\mathcal{A}$
% fig 23.1
\begin{center}
	\begin{tikzpicture}
		\node[state, initial]	(q0) {};
		\node[draw]	(l1) [above right=of q0] {DPDA $L_1$};
		\node[draw]	(l2) [below right=of q0] {DPDA $L_2$};

		\draw[->]	(q0)	to node[above] {$\epsilon$} (l1);
		\draw[->]	(q0)	to node[below] {$\epsilon$} (l2);
	\end{tikzpicture}
\end{center}
se la stringa non viene accettata, allora $\beta$ sicuramente non appartiene a $L(M)$.
Altrimenti se viene accettata abbiamo alcuni casi
\begin{itemize}
	\item se $\beta \in L_1 \cap L_2$ allora abbiamo 2 computazioni accettanti
	\item se $\beta \in (L_1 \setminus L_2$ o $\beta \in (L_2 \setminus L_1)$, allora c'è solo una computazione accettante
\end{itemize}
Allora vale che $\mathcal{A}$ è ambiguo sse $L_1 \cap L_2 = \varnothing$.
Quindi $\mathcal{A}$ non è ambiguo sse $L_1 \cap L_2 = \varnothing = \text{valid}(M)$ sse $L(M) = \varnothing$.
\begin{teorema}
	Non esiste algoritmo che, dato un PDA $\mathcal{A}$, decide se $\mathcal{A}$ è ambiguo.
\end{teorema}
Lo stesso vale per le grammatiche CF.

% ripetizione
Dati due linguaggi $L_1$ e $L_2$, il loro quoziente è definito come
$$ L_1 / L_2 = \{ x \in \Sigma^* \mid \exists y \in L_2 . xy \in L1 \} $$
Supponiamo che $L_2 = \{ z \}$, allora
$$ L / z = \{ x \in \Sigma^* \mid xz \in L \} $$
avevamo dimostrato che se $L_1$ e $L_2$ sono regolari, allora anche il loro quoziente è regolare.

%% problema della regolarità
Definiamo il problema della regolarità, come il problema che dato un linguaggio $L \in \text{CFL}$, ci domandiamo se $L$ è regolare.
Prendiamo $G = \langle V, \Sigma, P, S \rangle$ una grammatica CF.
Chiamiamo $L = L(G)$.	% diverso dall'L di sopra
Noi sappiamo che esistono dei linguaggi CF che non sono regolari, sia $L_0$ uno di questo, per cui
$$ L_0 \in \text{CFL} \setminus \text{Reg} $$
generato da una gramamtica $G_0$.
Costruiamo ora 
$$ L_1 = L_0 \# \Sigma^* \cup \Sigma^* \# L$$
con $\# \not \in \Sigma$.
Ora
\begin{itemize}
	\item se $L = \Sigma^*$, allora $L_1 = L \# \Sigma^* \cup \Sigma^* \# \Sigma^* = \Sigma^* \# \Sigma^*$, e questo linguaggio è regolare.
	\item se $L \neq \Sigma^*$, sia $w \not \in L$, calcoliamo
		$$ L_1 / \# w = L_0 $$
		siccome $w \not \in L$, tutte le stringhe di $\Sigma^* \# L$ non sono nel quoziente.
\end{itemize}
Visto che ??? sono chiusi rispetto al quoziente, allora $L_1 \in \text{CFL} \setminus \text{Reg}$.
Allora $L = \Sigma^*$ sse $L_1 \in \text{Reg}$.
Quindi se so decidere la regolarità di un CF, saprei decidere anche la sua universalità.

Definiamo ora il linguaggio delle computazioni valide su un input fissato
$$ \text{valid}(M, w), w \in \Sigma^* $$
Questo non cambia rispetto al $\text{valid}(M)$, e bisogna solo controllare che la computazione inizi per $w$.
Anche il suo complemento rimane invariato e vale che
\begin{align*}
	(\text{valid}(M, w))^C &= \Delta^* \setminus \text{valid}(M, w) \\
	                       &= \begin{cases} \Delta^* & \text{se } w \not \in L(M) \\ \Delta^* \setminus \{ p \} & \text{se } w \in L(M) \text{ e $p$ codifica la computazione che accetta $w$} \end{cases} \in \text{Reg}
\end{align*}
\begin{teorema}
	Non esiste un algoritmo che dato PDA o CFG per linguaggio regolare, produce un automa a stati finiti equivalente.
\end{teorema}
\begin{proof}
	Sia $M$ una macchina di Turing che riconosce un linguaggio ricorsivamente enuemrabile, ma non ricorsivo.
	Sia $w \in \Sigma^*$, costruisco la grammatica $G_w$ per $(\text{valid}(G, w))^C$, un linguaggio regolare.
	% ???, spento il puter
	\begin{itemize}
		\item se $w \in L(M)$, allora $L(G_w) = \Delta^* \setminus \{ \beta \} \neq \Delta^*$
		\item se $w \not \in L(M)$, allora $L(G_w) = \Delta^*$
	\end{itemize}
	vale quindi che $w \in L(M)$ sse $L(G_w) = \Delta^*$.
	Quindi non si può costruire l'automa a stati finiti.
\end{proof}

Vale inoltre che data una grammatica CF con $n$ simboli che genera un linguaggio regolare, non c'è limite al numero di stati che l'automa regolare corrispondente.
Infatti se ci fosse un upper bound potremmo esplorare tutti gli automi costruibili finché non si trova quello corretto.
Questo si chiama \textit{tradeoff}.
Abbiamo visto che passare da DFA nondeterminisitici a DFA deterministici a un tradeoff esponenziale. 
In questo secondo caso invece il tradeoff è illimitato.

Decidibilità
\begin{center}
	\begin{tblr}{colspec = { l | c c c} }
		&	Reg & DFCL & CF \\
		\hline
		\hline
		$L \overset{?}{=} \varnothing$ & D & D & D \\
		$L$ è finito? & D & D & D \\
		$L \overset{?}{=} \Delta^*$ & D & D & U \\	% visto che il complemento è decidibile per il DCFL
		$L_1 \overset{?}{=} L_2$ & D & D & U \\
		$L \overset{?}{\in} \text{Reg}$ & / & D & U \\	% se l'automa a pila è specificato con $n$ simboli, l'upper bound per l'FSA è 2^{2^{2^n}}, poi ridotto a 2^{2^n}
		$L_1 \cap L_2 \overset{?}{=} \varnothing$ & D & U & U \\
	\end{tblr}
\end{center}

 
\end{document}
